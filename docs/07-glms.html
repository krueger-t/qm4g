<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Generalised Linear Models (GLMs) | Quantitative Methods for Geographers</title>
  <meta name="description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Generalised Linear Models (GLMs) | Quantitative Methods for Geographers" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="github-repo" content="krueger-t/qm4g" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Generalised Linear Models (GLMs) | Quantitative Methods for Geographers" />
  
  <meta name="twitter:description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  

<meta name="author" content="Tobias Krueger" />


<meta name="date" content="2021-12-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="06-ml_bayes.html"/>
<link rel="next" href="08-multivariate.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block/empty-anchor.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding/rglWebGL.js"></script>
<link href="libs/rglwidgetClass/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass/rglClass.src.js"></script>
<script src="libs/rglwidgetClass/utils.src.js"></script>
<script src="libs/rglwidgetClass/subscenes.src.js"></script>
<script src="libs/rglwidgetClass/shaders.src.js"></script>
<script src="libs/rglwidgetClass/textures.src.js"></script>
<script src="libs/rglwidgetClass/projection.src.js"></script>
<script src="libs/rglwidgetClass/mouse.src.js"></script>
<script src="libs/rglwidgetClass/init.src.js"></script>
<script src="libs/rglwidgetClass/pieces.src.js"></script>
<script src="libs/rglwidgetClass/draw.src.js"></script>
<script src="libs/rglwidgetClass/controls.src.js"></script>
<script src="libs/rglwidgetClass/selection.src.js"></script>
<script src="libs/rglwidgetClass/rglTimer.src.js"></script>
<script src="libs/CanvasMatrix4/CanvasMatrix.src.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<link href="libs/bsTable/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable/bootstrapTable.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Geographers</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="02-math.html"><a href="02-math.html"><i class="fa fa-check"></i><b>1</b> Mathematical preliminaries</a></li>
<li class="chapter" data-level="2" data-path="03-lin_reg.html"><a href="03-lin_reg.html"><i class="fa fa-check"></i><b>2</b> Linear regression</a></li>
<li class="chapter" data-level="3" data-path="04-categorical_vars.html"><a href="04-categorical_vars.html"><i class="fa fa-check"></i><b>3</b> Categorical predictors</a></li>
<li class="chapter" data-level="4" data-path="05-multiple_lin_reg.html"><a href="05-multiple_lin_reg.html"><i class="fa fa-check"></i><b>4</b> Multiple linear regression</a></li>
<li class="chapter" data-level="5" data-path="06-ml_bayes.html"><a href="06-ml_bayes.html"><i class="fa fa-check"></i><b>5</b> Probabilistic underpinnings</a></li>
<li class="chapter" data-level="6" data-path="07-glms.html"><a href="07-glms.html"><i class="fa fa-check"></i><b>6</b> Generalised Linear Models (GLMs)</a></li>
<li class="chapter" data-level="7" data-path="08-multivariate.html"><a href="08-multivariate.html"><i class="fa fa-check"></i><b>7</b> Multivariate methods</a></li>
<li class="chapter" data-level="" data-path="09-solutions.html"><a href="09-solutions.html"><i class="fa fa-check"></i>Solutions to exercises</a></li>
<li class="chapter" data-level="" data-path="10-refs.html"><a href="10-refs.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Geographers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glms" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Generalised Linear Models (GLMs)</h1>
<p>Up to this point we have looked at the linear model
<span class="math display" id="eq:linmodi2">\[\begin{equation}
y_i = \beta_0 + \sum_{j=1}^{p}\beta_j \cdot x_{ij} + \epsilon_i
\tag{6.1}
\end{equation}\]</span>
with potentially multiple predictor variables <span class="math inline">\(x_j\)</span> that could be continuous, categorical or mixed. If <span class="math inline">\(\epsilon_i\sim N\left(\mu_i=0,\sigma\right)\)</span> then:
<span class="math display" id="eq:ynorm2">\[\begin{equation}
y_i\sim N\left(\mu_i=\beta_0 + \sum_{j=1}^{p}\beta_j \cdot x_{ij},\sigma\right)
\tag{6.2}
\end{equation}\]</span></p>
<p>So in effect we have modelled the response variable <span class="math inline">\(y_i\)</span> as realisations from a normal distribution whose mean <span class="math inline">\(\mu_i\)</span> (the mean response) is not a constant but a linear function of the predictors, hence the index <span class="math inline">\(i\)</span>. In this chapter - with Generalised Linear Models (GLMs) - we generalise this model to allow:</p>
<ol style="list-style-type: decimal">
<li>Distributions other than the normal distribution in Equation <a href="07-glms.html#eq:ynorm2">(6.2)</a></li>
<li>Transformations of the linear function linking predictors and mean response inside that distribution</li>
</ol>
<p>This is important as it allows us to deal with a number of shortcomings of linear regression. Equation <a href="07-glms.html#eq:ynorm2">(6.2)</a> implies that <span class="math inline">\(y_i\)</span> are <em>continuous</em> observations <em>between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span></em>, which is the so called support space of the normal distribution (Figure <a href="07-glms.html#fig:n">6.1</a>, left). Equation <a href="07-glms.html#eq:ynorm2">(6.2)</a> further implies that the variance <span class="math inline">\(Var\left(y_i\right)=\sigma^2\)</span> remains <em>constant</em> with changing mean response <span class="math inline">\(E\left[y_i\right]=\mu_i\)</span> (Figure <a href="07-glms.html#fig:n">6.1</a>, right).<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="07-glms.html#cb128-1"></a><span class="kw">library</span>(<span class="st">&quot;latex2exp&quot;</span>)</span>
<span id="cb128-2"><a href="07-glms.html#cb128-2"></a><span class="co"># generate y values from -5 to 5 in increments of 0.1</span></span>
<span id="cb128-3"><a href="07-glms.html#cb128-3"></a>y &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="fl">0.1</span>)</span>
<span id="cb128-4"><a href="07-glms.html#cb128-4"></a><span class="co"># plot pdf of normal distribution with mu=0 and sigma=1 for y</span></span>
<span id="cb128-5"><a href="07-glms.html#cb128-5"></a><span class="kw">plot</span>(y, <span class="kw">dnorm</span>(y, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>), <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">xlab=</span><span class="st">&#39;y&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;pdf&#39;</span>)</span>
<span id="cb128-6"><a href="07-glms.html#cb128-6"></a><span class="co"># plot schematic of constant variance for changing mean response</span></span>
<span id="cb128-7"><a href="07-glms.html#cb128-7"></a><span class="kw">plot</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb128-8"><a href="07-glms.html#cb128-8"></a>     <span class="dt">xlab=</span><span class="kw">TeX</span>(<span class="st">&#39;mean response $E</span><span class="ch">\\</span><span class="st">left[y_i</span><span class="ch">\\</span><span class="st">right]$&#39;</span>),</span>
<span id="cb128-9"><a href="07-glms.html#cb128-9"></a>     <span class="dt">ylab=</span><span class="kw">TeX</span>(<span class="st">&#39;variance $Var</span><span class="ch">\\</span><span class="st">left(y_i</span><span class="ch">\\</span><span class="st">right)$&#39;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:n"></span>
<img src="qm4g_files/figure-html/n-1.png" alt="Left: Probability density function (pdf) of the normal distribution of a response variable $y$ (for this schematic the standard normal distribution $N(\mu=0,\sigma=1)$ is shown). Right: In linear regression the variance of $y$ does not change with the mean response of $y$ as Equation \@ref(eq:ynorm2) implies." width="50%" /><img src="qm4g_files/figure-html/n-2.png" alt="Left: Probability density function (pdf) of the normal distribution of a response variable $y$ (for this schematic the standard normal distribution $N(\mu=0,\sigma=1)$ is shown). Right: In linear regression the variance of $y$ does not change with the mean response of $y$ as Equation \@ref(eq:ynorm2) implies." width="50%" />
<p class="caption">
Figure 6.1: Left: Probability density function (pdf) of the normal distribution of a response variable <span class="math inline">\(y\)</span> (for this schematic the standard normal distribution <span class="math inline">\(N(\mu=0,\sigma=1)\)</span> is shown). Right: In linear regression the variance of <span class="math inline">\(y\)</span> does not change with the mean response of <span class="math inline">\(y\)</span> as Equation <a href="07-glms.html#eq:ynorm2">(6.2)</a> implies.
</p>
</div>
<p>These two assumption, however, often do not hold for geographical data, e.g. in case of:</p>
<ul>
<li>Continuous observations restricted to positive outcomes</li>
<li>Data bounded over a fixed interval</li>
<li>Non-constant variance</li>
<li>Discrete data such as counts and proportions</li>
</ul>
<p>To some extent these shortcomings can be solved by transforming the data. But often it will be better to extend the linear model in the form of a GLM. In this course we use GLMs that are based on mathematics similar to the linear model to estimate the parameters, so we do not go into the particularities here. In my course <em>Applied Statistical Modelling</em> in the summer term we extend the realm of possible model choices even further.</p>
<p>In this chapter we will deal with two examples, one of count data and another of proportion data. The <strong>count data</strong> example is from <span class="citation">Piegorsch and Bailer (<a href="#ref-piegorsch2005" role="doc-biblioref">2005</a>)</span>; it models the number of bird species on the Indian subcontinent as a function of average altitudinal relief (Figure <a href="07-glms.html#fig:countdat">6.2</a>, left).<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> These data are <em>bounded below</em>; we cannot have counts less than zero. The residual variance is <em>non-constant</em> (Figure <a href="07-glms.html#fig:countdat">6.2</a>, right). The residual error is <em>non-normal</em>. And the data require a <em>discrete</em> distribution.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="07-glms.html#cb129-1"></a><span class="co"># load birds data</span></span>
<span id="cb129-2"><a href="07-glms.html#cb129-2"></a>birds &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/birds.txt&quot;</span>,<span class="dt">header=</span>T)</span>
<span id="cb129-3"><a href="07-glms.html#cb129-3"></a><span class="co"># plot</span></span>
<span id="cb129-4"><a href="07-glms.html#cb129-4"></a><span class="kw">plot</span>(birds<span class="op">$</span>relief, birds<span class="op">$</span>y, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">600</span>),</span>
<span id="cb129-5"><a href="07-glms.html#cb129-5"></a>     <span class="dt">xlab=</span><span class="st">&#39;Average relief&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Number of bird species&#39;</span>)</span>
<span id="cb129-6"><a href="07-glms.html#cb129-6"></a><span class="co"># plot schematic of increasing variance for increasing mean response</span></span>
<span id="cb129-7"><a href="07-glms.html#cb129-7"></a><span class="kw">plot</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">600</span>), <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">600</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb129-8"><a href="07-glms.html#cb129-8"></a>     <span class="dt">xlab=</span><span class="kw">TeX</span>(<span class="st">&#39;mean response $E</span><span class="ch">\\</span><span class="st">left[y_i</span><span class="ch">\\</span><span class="st">right]$&#39;</span>),</span>
<span id="cb129-9"><a href="07-glms.html#cb129-9"></a>     <span class="dt">ylab=</span><span class="kw">TeX</span>(<span class="st">&#39;variance $Var</span><span class="ch">\\</span><span class="st">left(y_i</span><span class="ch">\\</span><span class="st">right)$&#39;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:countdat"></span>
<img src="qm4g_files/figure-html/countdat-1.png" alt="Left: Number of bird species as a function of average relief. Data from: @piegorsch2005. Right: In this example the variance of $y$ increases with the mean response of $y$." width="50%" /><img src="qm4g_files/figure-html/countdat-2.png" alt="Left: Number of bird species as a function of average relief. Data from: @piegorsch2005. Right: In this example the variance of $y$ increases with the mean response of $y$." width="50%" />
<p class="caption">
Figure 6.2: Left: Number of bird species as a function of average relief. Data from: <span class="citation">Piegorsch and Bailer (<a href="#ref-piegorsch2005" role="doc-biblioref">2005</a>)</span>. Right: In this example the variance of <span class="math inline">\(y\)</span> increases with the mean response of <span class="math inline">\(y\)</span>.
</p>
</div>
<p>The <strong>proportion data</strong> example is from <span class="citation">Dormann (<a href="#ref-dormann2013" role="doc-biblioref">2013</a>)</span>; it models the proportion of organisms surviving as a function of the concentration of a toxic substance (Figure <a href="07-glms.html#fig:survivaldat">6.3</a>, left). These data are <em>strictly bounded</em>; we cannot have a proportion greater than 1 or less than 0. The residual variance is <em>non-constant</em> (Figure <a href="07-glms.html#fig:survivaldat">6.3</a>, right). And the residual error is <em>non-normal</em>; confidence intervals are asymmetric whenever predictions are large (close to 1) or small (close to 0).</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="07-glms.html#cb130-1"></a><span class="co"># load survival data</span></span>
<span id="cb130-2"><a href="07-glms.html#cb130-2"></a>survival &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/survival.txt&quot;</span>,<span class="dt">header=</span>T)</span>
<span id="cb130-3"><a href="07-glms.html#cb130-3"></a><span class="co"># these data are given in the form &quot;trials&quot; (here total no. organisms)</span></span>
<span id="cb130-4"><a href="07-glms.html#cb130-4"></a><span class="co">#  and &quot;successes&quot; (here no. organisms surviving)</span></span>
<span id="cb130-5"><a href="07-glms.html#cb130-5"></a><span class="co">#  so transform into proportions</span></span>
<span id="cb130-6"><a href="07-glms.html#cb130-6"></a>survival<span class="op">$</span>proportion &lt;-<span class="st"> </span>survival<span class="op">$</span>surviving <span class="op">/</span><span class="st"> </span>survival<span class="op">$</span>total</span>
<span id="cb130-7"><a href="07-glms.html#cb130-7"></a><span class="co"># plot</span></span>
<span id="cb130-8"><a href="07-glms.html#cb130-8"></a><span class="kw">plot</span>(survival<span class="op">$</span>concentration, survival<span class="op">$</span>proportion, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb130-9"><a href="07-glms.html#cb130-9"></a>     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">xlab=</span><span class="st">&#39;Concentration (mg/l)&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Proportion&#39;</span>)</span>
<span id="cb130-10"><a href="07-glms.html#cb130-10"></a><span class="co"># plot schematic of variance peaking at average mean response</span></span>
<span id="cb130-11"><a href="07-glms.html#cb130-11"></a><span class="kw">plot</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="fl">0.1</span>), <span class="op">-</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="fl">0.1</span>)<span class="op">^</span><span class="dv">2</span>, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">25</span>,<span class="dv">0</span>),</span>
<span id="cb130-12"><a href="07-glms.html#cb130-12"></a>     <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">xlab=</span><span class="kw">TeX</span>(<span class="st">&#39;mean response $E</span><span class="ch">\\</span><span class="st">left[y_i</span><span class="ch">\\</span><span class="st">right]$&#39;</span>),</span>
<span id="cb130-13"><a href="07-glms.html#cb130-13"></a>     <span class="dt">ylab=</span><span class="kw">TeX</span>(<span class="st">&#39;variance $Var</span><span class="ch">\\</span><span class="st">left(y_i</span><span class="ch">\\</span><span class="st">right)$&#39;</span>))</span>
<span id="cb130-14"><a href="07-glms.html#cb130-14"></a><span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">at=</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">2</span>),<span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;0&quot;</span>,<span class="st">&quot;0.2&quot;</span>,<span class="st">&quot;0.4&quot;</span>,<span class="st">&quot;0.6&quot;</span>,<span class="st">&quot;0.8&quot;</span>,<span class="st">&quot;1&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:survivaldat"></span>
<img src="qm4g_files/figure-html/survivaldat-1.png" alt="Left: Proportion of organisms surviving as a function of the concentration of a toxic substance. Data from: @dormann2013. Right: In this example the variance of $y$ peaks at the mean response of $y$." width="50%" /><img src="qm4g_files/figure-html/survivaldat-2.png" alt="Left: Proportion of organisms surviving as a function of the concentration of a toxic substance. Data from: @dormann2013. Right: In this example the variance of $y$ peaks at the mean response of $y$." width="50%" />
<p class="caption">
Figure 6.3: Left: Proportion of organisms surviving as a function of the concentration of a toxic substance. Data from: <span class="citation">Dormann (<a href="#ref-dormann2013" role="doc-biblioref">2013</a>)</span>. Right: In this example the variance of <span class="math inline">\(y\)</span> peaks at the mean response of <span class="math inline">\(y\)</span>.
</p>
</div>
<div id="generalising-the-normal-distribution-through-the-exponential-class-of-probability-functions" class="section level2">
<h2><span class="header-section-number">6.1</span> Generalising the normal distribution through the exponential class of probability functions</h2>
<p>In this section we see how the normal distribution of linear regression can be generalised via the exponential class of probability functions. The normal distribution is a member of this class and so are other distributions, like the Poisson and the binomial that we need for our examples. It can then be shown - which we do not do here - that the mathematics of linear regression can be extended to those other distributional assumptions. This is not to say that distributions that are not in the exponential class cannot be used for regression analysis; it is just that such regression problems are not so straightforward to solve in classical statistics.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></p>
<p>The <strong>exponential class of probability functions</strong> is formalised as:
<span class="math display" id="eq:expclass">\[\begin{equation}
f(y)=\exp\left\{\frac{y\cdot\theta-\color{red}{B(\theta)}}{\color{green}{A(\varphi)}}+\color{blue}{C(y,\varphi)}\right\}
\tag{6.3}
\end{equation}\]</span>
With the additional constraint that the support space <span class="math inline">\(\mathbb{S}\)</span> of <span class="math inline">\(y\)</span> cannot depend on <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math inline">\(\color{green}{A(\varphi)}\)</span>, <span class="math inline">\(\color{red}{B(\theta)}\)</span> and <span class="math inline">\(\color{blue}{C(y,\varphi)}\)</span> are functions that take on different forms for different members of the exponential class. The parameter <span class="math inline">\(\theta\)</span> is called the <strong>natural parameter</strong>, and the parameter <span class="math inline">\(\varphi&gt;0\)</span> is called the <strong>dispersion (or scale) parameter</strong>.</p>
<p>We can now transform the pdf of the <strong>normal distribution</strong> (Figure <a href="07-glms.html#fig:n">6.1</a>, left) to the form of Equation <a href="07-glms.html#eq:expclass">(6.3)</a> to demonstrate how the normal distribution is a member of the exponential class, and which the natural and dispersion parameters are in this case:</p>
<p><span class="math display" id="eq:npdf">\[\begin{equation}
f(y)=\frac{1}{\sigma\cdot\sqrt{2\cdot\pi}}\cdot\exp{-\frac{(y-\mu)^2}{2\cdot\sigma^2}}\cdot I_{(-\infty,\infty)}(y)
\tag{6.4}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:expclassn">\[\begin{equation}
f(y)=\exp\left\{\frac{y\cdot\mu-\color{red}{\frac{1}{2}\cdot\mu}}{\color{green}{\sigma^2}}\color{blue}{-\frac{y^2}{2\cdot\sigma^2}-\frac{1}{2}\cdot\log\left(2\cdot\pi\cdot\sigma^2\right)+\log\left[I_{(-\infty,\infty)}(y)\right]}\right\}
\tag{6.5}
\end{equation}\]</span></p>
<p>Note, the indicator function <span class="math inline">\(I_{(-\infty,\infty)}(y)\)</span> is 1 when <span class="math inline">\(y\)</span> is between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span> and 0 otherwise, indicating the support space <span class="math inline">\(\mathbb{S}\)</span>. By comparing Equation <a href="07-glms.html#eq:expclass">(6.3)</a> and Equation <a href="07-glms.html#eq:expclassn">(6.5)</a> we can see that for the normal distribution the natural parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\mu\)</span> and the dispersion parameter <span class="math inline">\(\varphi\)</span> is <span class="math inline">\(\sigma\)</span>. The mean response is <span class="math inline">\(E[y]=\mu\)</span>.</p>
<p>Next we see how the <strong>Poisson distribution</strong> (Figure <a href="07-glms.html#fig:poisb">6.4</a>, left), which we will need for <strong>count data</strong>, is also a member of the exponential class. The pdf of the Poisson distribution is:</p>
<p><span class="math display" id="eq:poispdf">\[\begin{equation}
f(y)=\frac{\lambda^y\cdot\exp(-\lambda)}{y!}\cdot I_{(0,1,\ldots,n)}(y)
\tag{6.6}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:expclasspois">\[\begin{equation}
f(y)=\exp\left\{y\cdot\log(\lambda)-\color{red}{\lambda}\color{blue}{-\log(y!)+\log\left[y!\cdot I_{(0,1,\ldots,n)}(y)\right]}\right\}
\tag{6.7}
\end{equation}\]</span></p>
<p>Note, the symbol <span class="math inline">\(!\)</span> stands for the factorial.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> The support space <span class="math inline">\(\mathbb{S}\)</span> is here restricted to positive whole numbers. By comparing Equation <a href="07-glms.html#eq:expclass">(6.3)</a> and Equation <a href="07-glms.html#eq:expclasspois">(6.7)</a> we can see that for the Poisson distribution the natural parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\log(\lambda)\)</span> and the dispersion parameter <span class="math inline">\(\varphi\)</span> is 1; the denominator in Equation <a href="07-glms.html#eq:expclass">(6.3)</a> has vanished in Equation <a href="07-glms.html#eq:expclasspois">(6.7)</a>. The mean response is <span class="math inline">\(E[y]=\lambda\)</span>.</p>
<p>The one parameter of the Poisson distribution <span class="math inline">\(\lambda\)</span> controls both the mean as well as the variance of the distribution, i.e. if the mean goes up then the variance goes up too (Figure <a href="07-glms.html#fig:poisb">6.4</a>, left). Not only is the Poisson distribution, due to its discreteness and positive support space, a natural distribution for count data, it can also capture the increase in variance with increase in mean response that we often see in count data such as our bird species example (Figure <a href="07-glms.html#fig:countdat">6.2</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:poisb"></span>
<img src="qm4g_files/figure-html/poisb-1.png" alt="Left: Probability density function (pdf) of the **Poisson distribution** of a response variable $y$ for three variants of the parameter $\lambda$. Right: Probability density function (pdf) of the **binomial distribution** of a response variable $y$, normalised by the number of &quot;trials&quot; $n$, for three variants of the parameter $\pi$." width="50%" /><img src="qm4g_files/figure-html/poisb-2.png" alt="Left: Probability density function (pdf) of the **Poisson distribution** of a response variable $y$ for three variants of the parameter $\lambda$. Right: Probability density function (pdf) of the **binomial distribution** of a response variable $y$, normalised by the number of &quot;trials&quot; $n$, for three variants of the parameter $\pi$." width="50%" />
<p class="caption">
Figure 6.4: Left: Probability density function (pdf) of the <strong>Poisson distribution</strong> of a response variable <span class="math inline">\(y\)</span> for three variants of the parameter <span class="math inline">\(\lambda\)</span>. Right: Probability density function (pdf) of the <strong>binomial distribution</strong> of a response variable <span class="math inline">\(y\)</span>, normalised by the number of “trials” <span class="math inline">\(n\)</span>, for three variants of the parameter <span class="math inline">\(\pi\)</span>.
</p>
</div>
<p>Lastly, the <strong>binomial distribution</strong> (Figure <a href="07-glms.html#fig:poisb">6.4</a>, right), which we will need for <strong>proportion data</strong>, is also a member of the exponential class. The pdf of the binomial distribution is:</p>
<p><span class="math display" id="eq:bpdf">\[\begin{equation}
f(y)=\binom{n}{y}\cdot\pi^y\cdot (1-\pi)^{n-y}\cdot I_{(0,1,\ldots,n)}(y)
\tag{6.8}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:expclassb">\[\begin{equation}
f(y)=\exp\left\{y\cdot\log(\frac{\pi}{1-\pi})\color{red}{+n\cdot\log(1-\pi)}+\color{blue}{\log\left[\binom{n}{y}\cdot I_{(0,1,\ldots,n)}(y)\right]}\right\}
\tag{6.9}
\end{equation}\]</span></p>
<p>Note, <span class="math inline">\(\binom{n}{y}\)</span> stands for the binomial coefficient.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> The support space <span class="math inline">\(\mathbb{S}\)</span> is again restricted to positive whole numbers. By comparing Equation <a href="07-glms.html#eq:expclass">(6.3)</a> and Equation <a href="07-glms.html#eq:expclassb">(6.9)</a> we can see that for the binomial distribution the natural parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\log(\frac{\pi}{1-\pi})\)</span>, the so called <strong>logit function</strong> or <strong>log-odds</strong><a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> and the dispersion parameter <span class="math inline">\(\varphi\)</span> is again 1. The mean response is <span class="math inline">\(E[\frac{y}{n}]=\pi\)</span>. Note, here we normalise the response <span class="math inline">\(y\)</span> by the number of “trials” <span class="math inline">\(n\)</span> to deal with proportions, while the binomial distribution <em>per se</em> is a count data distribution.</p>
<p>The one parameter of the binomial distribution <span class="math inline">\(\pi\)</span> controls the mean, the variance as well as the shape of the distribution (Figure <a href="07-glms.html#fig:poisb">6.4</a>, right). It thus captures the change in variance with change in mean response as well as the asymmetry of the response distribution near 1 and 0 that we see in strictly bounded data such as our survival proportions example (Figure <a href="07-glms.html#fig:survivaldat">6.3</a>).</p>
</div>
<div id="the-link-function-between-mean-response-and-predictor-variables" class="section level2">
<h2><span class="header-section-number">6.2</span> The link function between mean response and predictor variables</h2>
<p>Now we come to the second move of GLMs, the link function between mean response and predictor variables. To understand this, we first introduce the symbol <span class="math inline">\(\eta_i\)</span> as shorthand for the <strong>linear predictor</strong>:
<span class="math display" id="eq:linpred">\[\begin{equation}
\eta_i = \beta_0 + \sum_{j=1}^{p}\beta_j \cdot x_{ij}
\tag{6.10}
\end{equation}\]</span></p>
<p>The <strong>link function</strong> <span class="math inline">\(g(\cdot)\)</span> then relates the linear predictor to the mean response:
<span class="math display" id="eq:link">\[\begin{equation}
\eta_i = g\left(E\left[y_i\right]\right)
\tag{6.11}
\end{equation}\]</span>
Remember that in linear regression the linear predictor was simply equated with the parameter <span class="math inline">\(\mu_i\)</span> (Equation <a href="07-glms.html#eq:ynorm2">(6.2)</a>), which is just the mean response <span class="math inline">\(E\left[y_i\right]\)</span> of the normal model. This is called the <strong>identity link</strong>: <span class="math inline">\(\eta_i=E\left[y_i\right]=\mu_i\)</span>.</p>
<p>The <strong>inverse link function</strong> <span class="math inline">\(g^{-1}(\cdot)\)</span> then solves the same equation for the mean response:
<span class="math display" id="eq:invlink">\[\begin{equation}
E\left[y_i\right]=g^{-1}\left(\eta_i\right)
\tag{6.12}
\end{equation}\]</span></p>
<p>This formulation of the mean response is eventually entered into the assumed <strong>response distribution</strong>, e.g. in case of the normal distribution (compare Equation <a href="07-glms.html#eq:ynorm2">(6.2)</a>): <span class="math inline">\(y_i\sim N\left(E\left[y_i\right]=\mu_i=\eta_i,\sigma\right)\)</span>. Note, it is only ever the inverse link-function that is mathematically used in the model, not the link function itself.</p>
<p>The choice of link function is best motivated by our mechanistic understanding of the process to be modelled. If not, then the natural parameter <span class="math inline">\(\theta\)</span> - whatever it is for the response distribution we are using - may be chosen as the link function – this is called the <strong>canonical link function</strong> (Table <a href="07-glms.html#tab:links">6.1</a>).</p>
<table>
<caption><span id="tab:links">Table 6.1: </span> Canonical link functions.</caption>
<thead>
<tr class="header">
<th align="left">Response distribution</th>
<th align="left">Canonical link function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Normal</td>
<td align="left">Identity-link: <span class="math inline">\(\eta_i=\mu_i\)</span></td>
</tr>
<tr class="even">
<td align="left">Poisson</td>
<td align="left">Log-link: <span class="math inline">\(\eta_i=\log\left(\lambda_i\right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Binomial</td>
<td align="left">Logit-link: <span class="math inline">\(\eta_i=\log\left\{\frac{\pi_i}{1-\pi_i}\right\}\)</span></td>
</tr>
</tbody>
</table>
<p>The resultant GLMs are compared in Table <a href="07-glms.html#tab:glms">6.2</a>.</p>
<table>
<caption><span id="tab:glms">Table 6.2: </span> Components of three widely used GLMs.</caption>
<colgroup>
<col width="21%" />
<col width="26%" />
<col width="26%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Normal</th>
<th align="center">Poisson</th>
<th align="center">Binomial</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear predictor</td>
<td align="center"><span class="math inline">\(\eta_i = \beta_0 + \sum_{j=1}^{p}\beta_j \cdot x_{ij}\)</span></td>
<td align="center"><span class="math inline">\(\eta_i = \beta_0 + \sum_{j=1}^{p}\beta_j \cdot x_{ij}\)</span></td>
<td align="center"><span class="math inline">\(\eta_i = \beta_0 + \sum_{j=1}^{p}\beta_j \cdot x_{ij}\)</span></td>
</tr>
<tr class="even">
<td align="left">Response distribution</td>
<td align="center"><span class="math inline">\(N\left(\mu_i,\sigma\right)\)</span></td>
<td align="center"><span class="math inline">\(Pois\left(\lambda_i\right)\)</span></td>
<td align="center"><span class="math inline">\(B\left(n,\pi_i\right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Mean response</td>
<td align="center"><span class="math inline">\(E\left[y_i\right]=\mu_i\)</span></td>
<td align="center"><span class="math inline">\(E\left[y_i\right]=\lambda_i\)</span></td>
<td align="center"><span class="math inline">\(E\left[\frac{y_i}{n}\right]=\pi_i\)</span></td>
</tr>
<tr class="even">
<td align="left">Link function</td>
<td align="center"><span class="math inline">\(\eta_i=\mu_i\)</span></td>
<td align="center"><span class="math inline">\(\eta_i=\log\left(\lambda_i\right)\)</span></td>
<td align="center"><span class="math inline">\(\eta_i=\log\left\{\frac{\pi_i}{1-\pi_i}\right\}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Inverse link function</td>
<td align="center"><span class="math inline">\(\mu_i=\eta_i\)</span></td>
<td align="center"><span class="math inline">\(\lambda_i=\exp(\eta_i)\)</span></td>
<td align="center"><span class="math inline">\(\pi_i=\frac{\exp\left(\eta_i\right)}{1+\exp\left(\eta_i\right)}\)</span></td>
</tr>
<tr class="even">
<td align="left">GLM</td>
<td align="center"><span class="math inline">\(N\left(\eta_i,\sigma\right)\)</span></td>
<td align="center"><span class="math inline">\(Pois\left(\exp\left(\eta_i\right)\right)\)</span></td>
<td align="center"><span class="math inline">\(B\left(n,\frac{\exp\left(\eta_i\right)}{1+\exp\left(\eta_i\right)}\right)\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">Linear regression</td>
<td align="center">Log-linear regression</td>
<td align="center">Logistic regression</td>
</tr>
</tbody>
</table>
</div>
<div id="log-linear-regression" class="section level2">
<h2><span class="header-section-number">6.3</span> Log-linear regression</h2>
<p>Regression analysis with the <em>Poisson distribution</em> and <em>log-link</em> is called <strong>log-linear regression</strong>. Let us look at this for the bird species dataset (Figure <a href="07-glms.html#fig:loglin">6.5</a>, left). Note that it is the linear predictor <span class="math inline">\(\eta_i\)</span> that gets transformed non-linearly (through <span class="math inline">\(g^{-1}(\cdot)\)</span>), not the data <span class="math inline">\(y_i\)</span>! Nevertheless, we can transform the data (through <span class="math inline">\(g(\cdot)\)</span>) to get an idea of what happens (Figure <a href="07-glms.html#fig:loglin">6.5</a>, right).</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="07-glms.html#cb131-1"></a><span class="co"># fit log-linear model to birds data</span></span>
<span id="cb131-2"><a href="07-glms.html#cb131-2"></a>birds_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>relief, <span class="dt">family =</span> <span class="kw">poisson</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>), <span class="dt">data =</span> birds)</span>
<span id="cb131-3"><a href="07-glms.html#cb131-3"></a><span class="co"># plot data</span></span>
<span id="cb131-4"><a href="07-glms.html#cb131-4"></a><span class="kw">plot</span>(birds<span class="op">$</span>relief, birds<span class="op">$</span>y, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">600</span>),</span>
<span id="cb131-5"><a href="07-glms.html#cb131-5"></a>     <span class="dt">xlab=</span><span class="st">&#39;Average relief&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Number of bird species&#39;</span>)</span>
<span id="cb131-6"><a href="07-glms.html#cb131-6"></a><span class="co"># extract modelled mean response and plot on top of data</span></span>
<span id="cb131-7"><a href="07-glms.html#cb131-7"></a>newdat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">relief=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="fl">0.01</span>))</span>
<span id="cb131-8"><a href="07-glms.html#cb131-8"></a>y_new &lt;-<span class="st"> </span><span class="kw">predict</span>(birds_fit, <span class="dt">newdata=</span>newdat, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb131-9"><a href="07-glms.html#cb131-9"></a><span class="kw">lines</span>(newdat<span class="op">$</span>relief, y_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb131-10"><a href="07-glms.html#cb131-10"></a><span class="co"># plot data on log-scale</span></span>
<span id="cb131-11"><a href="07-glms.html#cb131-11"></a><span class="kw">plot</span>(birds<span class="op">$</span>relief, <span class="kw">log</span>(birds<span class="op">$</span>y), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">4.5</span>,<span class="fl">6.5</span>),</span>
<span id="cb131-12"><a href="07-glms.html#cb131-12"></a>     <span class="dt">xlab=</span><span class="st">&#39;Average relief&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;log(Number of bird species)&#39;</span>)</span>
<span id="cb131-13"><a href="07-glms.html#cb131-13"></a><span class="co"># extract modelled mean linear predictor and plot on top of log-data</span></span>
<span id="cb131-14"><a href="07-glms.html#cb131-14"></a>logy_new &lt;-<span class="st"> </span><span class="kw">predict</span>(birds_fit, <span class="dt">newdata=</span>newdat, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>)</span>
<span id="cb131-15"><a href="07-glms.html#cb131-15"></a><span class="kw">lines</span>(newdat<span class="op">$</span>relief, logy_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:loglin"></span>
<img src="qm4g_files/figure-html/loglin-1.png" alt="Left: Bird species data with fitted log-linear model. The linear predictor is transformed via the inverse log-link to curve upwards. Right: If we transform the response data using the log-link then we can see the linear predictor at the heart of the model." width="50%" /><img src="qm4g_files/figure-html/loglin-2.png" alt="Left: Bird species data with fitted log-linear model. The linear predictor is transformed via the inverse log-link to curve upwards. Right: If we transform the response data using the log-link then we can see the linear predictor at the heart of the model." width="50%" />
<p class="caption">
Figure 6.5: Left: Bird species data with fitted log-linear model. The linear predictor is transformed via the inverse log-link to curve upwards. Right: If we transform the response data using the log-link then we can see the linear predictor at the heart of the model.
</p>
</div>
<p>To get a sense of what is happening here, consider Figure <a href="07-glms.html#fig:birds">6.6</a>. In order to represent the variation in the response we could use one overall Poisson distribution (a), which is equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. By introducing “relief” as a predictor we effectively model every relief level with its own Poisson distribution. This is especially easy to show here as the dataset has only four levels (b)-(e). What log-linear regression does is average those individual Poisson distributions to form an overall distribution of the response (f). This is clearly better at capturing the variation in the data, the predictor “relief” explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response; this is especially visible in (d)-(e). We say the data are <em>over-dispersed</em> with respect to the Poisson process, a topic we will come back to in Chapter <a href="07-glms.html#overdisp">6.6</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:birds"></span>
<img src="qm4g_files/figure-html/birds-1.png" alt="Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure \@ref(fig:loglin)) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor &quot;relief&quot; explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are _over-dispersed_ with respect to the Poisson process (see Chapter \@ref(overdisp))." width="33%" /><img src="qm4g_files/figure-html/birds-2.png" alt="Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure \@ref(fig:loglin)) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor &quot;relief&quot; explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are _over-dispersed_ with respect to the Poisson process (see Chapter \@ref(overdisp))." width="33%" /><img src="qm4g_files/figure-html/birds-3.png" alt="Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure \@ref(fig:loglin)) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor &quot;relief&quot; explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are _over-dispersed_ with respect to the Poisson process (see Chapter \@ref(overdisp))." width="33%" /><img src="qm4g_files/figure-html/birds-4.png" alt="Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure \@ref(fig:loglin)) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor &quot;relief&quot; explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are _over-dispersed_ with respect to the Poisson process (see Chapter \@ref(overdisp))." width="33%" /><img src="qm4g_files/figure-html/birds-5.png" alt="Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure \@ref(fig:loglin)) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor &quot;relief&quot; explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are _over-dispersed_ with respect to the Poisson process (see Chapter \@ref(overdisp))." width="33%" /><img src="qm4g_files/figure-html/birds-6.png" alt="Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure \@ref(fig:loglin)) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor &quot;relief&quot; explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are _over-dispersed_ with respect to the Poisson process (see Chapter \@ref(overdisp))." width="33%" />
<p class="caption">
Figure 6.6: Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly does not capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure <a href="07-glms.html#fig:loglin">6.5</a>) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor “relief” explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are <em>over-dispersed</em> with respect to the Poisson process (see Chapter <a href="07-glms.html#overdisp">6.6</a>).
</p>
</div>
<p>A <em>multiple regression</em> example of the log-linear model is the number of plant species as a function of biomass (a continuous predictor) and soil pH class (a categorical predictor) which you can find in <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span>, section 14.3, p. 586ff.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">6.4</span> Logistic regression</h2>
<p>Regression analysis with the <em>binomial distribution</em> and <em>logit-link</em> is called <strong>logistic regression</strong>. Let us look at this for the survival dataset (Figure <a href="07-glms.html#fig:logistic">6.7</a>, left). Again, it is the linear predictor <span class="math inline">\(\eta_i\)</span> that gets transformed non-linearly (through <span class="math inline">\(g^{-1}(\cdot)\)</span>), not the data <span class="math inline">\(y_i\)</span>, but we can transform the data (through <span class="math inline">\(g(\cdot)\)</span>) to get an idea of what happens (Figure <a href="07-glms.html#fig:logistic">6.7</a>, right)!</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="07-glms.html#cb132-1"></a><span class="co"># fit logistic model to survival data</span></span>
<span id="cb132-2"><a href="07-glms.html#cb132-2"></a><span class="co"># note, this is the proportions &amp; weights formulation;</span></span>
<span id="cb132-3"><a href="07-glms.html#cb132-3"></a><span class="co">#  there are two more ways to code the response</span></span>
<span id="cb132-4"><a href="07-glms.html#cb132-4"></a>survival_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(proportion <span class="op">~</span><span class="st"> </span>concentration, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb132-5"><a href="07-glms.html#cb132-5"></a>                    <span class="dt">data =</span> survival, <span class="dt">weights =</span> total)</span>
<span id="cb132-6"><a href="07-glms.html#cb132-6"></a><span class="co"># plot data</span></span>
<span id="cb132-7"><a href="07-glms.html#cb132-7"></a><span class="kw">plot</span>(survival<span class="op">$</span>concentration, survival<span class="op">$</span>proportion, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb132-8"><a href="07-glms.html#cb132-8"></a>     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">xlab=</span><span class="st">&#39;Concentration (mg/l)&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Proportion&#39;</span>)</span>
<span id="cb132-9"><a href="07-glms.html#cb132-9"></a><span class="co"># extract modelled mean response and plot on top of data</span></span>
<span id="cb132-10"><a href="07-glms.html#cb132-10"></a>newdat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">concentration=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">4</span>,<span class="fl">0.01</span>))</span>
<span id="cb132-11"><a href="07-glms.html#cb132-11"></a>y_new &lt;-<span class="st"> </span><span class="kw">predict</span>(survival_fit, <span class="dt">newdata=</span>newdat, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb132-12"><a href="07-glms.html#cb132-12"></a><span class="kw">lines</span>(newdat<span class="op">$</span>concentration, y_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb132-13"><a href="07-glms.html#cb132-13"></a><span class="co"># plot data on logit-scale</span></span>
<span id="cb132-14"><a href="07-glms.html#cb132-14"></a><span class="kw">plot</span>(survival<span class="op">$</span>concentration, <span class="kw">log</span>(survival<span class="op">$</span>proportion<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>survival<span class="op">$</span>proportion)),</span>
<span id="cb132-15"><a href="07-glms.html#cb132-15"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">6</span>),</span>
<span id="cb132-16"><a href="07-glms.html#cb132-16"></a>     <span class="dt">xlab=</span><span class="st">&#39;Concentration (mg/l)&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;logit(Proportion)&#39;</span>)</span>
<span id="cb132-17"><a href="07-glms.html#cb132-17"></a><span class="co"># extract modelled mean linear predictor and plot on top of logit-data</span></span>
<span id="cb132-18"><a href="07-glms.html#cb132-18"></a>logity_new &lt;-<span class="st"> </span><span class="kw">predict</span>(survival_fit, <span class="dt">newdata=</span>newdat, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>)</span>
<span id="cb132-19"><a href="07-glms.html#cb132-19"></a><span class="kw">lines</span>(newdat<span class="op">$</span>concentration, logity_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:logistic"></span>
<img src="qm4g_files/figure-html/logistic-1.png" alt="Left: Survival data with fitted logistic model. The linear predictor is transformed via the inverse logit-link to form an inverted s-shape. Right: If we transform the response data using the logit-link then we can see the linear predictor at the heart of the model." width="50%" /><img src="qm4g_files/figure-html/logistic-2.png" alt="Left: Survival data with fitted logistic model. The linear predictor is transformed via the inverse logit-link to form an inverted s-shape. Right: If we transform the response data using the logit-link then we can see the linear predictor at the heart of the model." width="50%" />
<p class="caption">
Figure 6.7: Left: Survival data with fitted logistic model. The linear predictor is transformed via the inverse logit-link to form an inverted s-shape. Right: If we transform the response data using the logit-link then we can see the linear predictor at the heart of the model.
</p>
</div>
<p>The logistic model also works for <em>presence/absence data</em> of, for example, animal species. This type of data is a binary response variable that can only ever be 0 (absence) or 1 (presence). Nevertheless, we can fit a logistic regression to these data and thus model the <em>probability of presence</em> of the species as a function of some predictors.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> An example is the presence and absence of Bison as a function of forest distance, a dataset that was given to me by <a href="https://www.geographie.hu-berlin.de/en/professorships/biogeography/people/current-people/bleyhl">Benjamin Bleyhl</a> (Figure <a href="07-glms.html#fig:bison">6.8</a>). Another example is the presence and absence of the <em>Uta</em> lizard as a function of island perimeter/area ratio which you can find in <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>, section 13.2.1, p. 360ff.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="07-glms.html#cb133-1"></a><span class="co"># load bison data</span></span>
<span id="cb133-2"><a href="07-glms.html#cb133-2"></a>bison &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/bison.txt&quot;</span>,<span class="dt">header=</span>T)</span>
<span id="cb133-3"><a href="07-glms.html#cb133-3"></a><span class="co"># plot</span></span>
<span id="cb133-4"><a href="07-glms.html#cb133-4"></a><span class="kw">plot</span>(bison<span class="op">$</span>d2f, bison<span class="op">$</span>occurrence, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">6000</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb133-5"><a href="07-glms.html#cb133-5"></a>     <span class="dt">xlab=</span><span class="st">&#39;Distance to forest&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Bison occurrence&#39;</span>)</span>
<span id="cb133-6"><a href="07-glms.html#cb133-6"></a><span class="co"># fit logistic model</span></span>
<span id="cb133-7"><a href="07-glms.html#cb133-7"></a><span class="co"># note, this is the factor formulation,</span></span>
<span id="cb133-8"><a href="07-glms.html#cb133-8"></a><span class="co">#  which is the only one that makes sense for presence/absence</span></span>
<span id="cb133-9"><a href="07-glms.html#cb133-9"></a>bison_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(occurrence <span class="op">~</span><span class="st"> </span>d2f, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="dt">data =</span> bison)</span>
<span id="cb133-10"><a href="07-glms.html#cb133-10"></a><span class="co"># extract modelled mean response and plot on top of data</span></span>
<span id="cb133-11"><a href="07-glms.html#cb133-11"></a>newdat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">d2f=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">6000</span>,<span class="dv">10</span>))</span>
<span id="cb133-12"><a href="07-glms.html#cb133-12"></a>y_new &lt;-<span class="st"> </span><span class="kw">predict</span>(bison_fit, <span class="dt">newdata=</span>newdat, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb133-13"><a href="07-glms.html#cb133-13"></a><span class="kw">lines</span>(newdat<span class="op">$</span>d2f, y_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb133-14"><a href="07-glms.html#cb133-14"></a><span class="co"># generate binned averages of response</span></span>
<span id="cb133-15"><a href="07-glms.html#cb133-15"></a><span class="co"># after: https://avehtari.github.io/ROS-Examples/LogitGraphs/logitgraphs.html</span></span>
<span id="cb133-16"><a href="07-glms.html#cb133-16"></a>K &lt;-<span class="st"> </span><span class="dv">60</span></span>
<span id="cb133-17"><a href="07-glms.html#cb133-17"></a>bins &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">cut</span>(bison<span class="op">$</span>d2f, K))</span>
<span id="cb133-18"><a href="07-glms.html#cb133-18"></a>x_bar &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, K)</span>
<span id="cb133-19"><a href="07-glms.html#cb133-19"></a>y_bar &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, K)</span>
<span id="cb133-20"><a href="07-glms.html#cb133-20"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K){</span>
<span id="cb133-21"><a href="07-glms.html#cb133-21"></a>  x_bar[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(bison<span class="op">$</span>d2f[bins<span class="op">==</span>k])</span>
<span id="cb133-22"><a href="07-glms.html#cb133-22"></a>  y_bar[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(bison<span class="op">$</span>occurrence[bins<span class="op">==</span>k])</span>
<span id="cb133-23"><a href="07-glms.html#cb133-23"></a>}</span>
<span id="cb133-24"><a href="07-glms.html#cb133-24"></a><span class="co"># plot</span></span>
<span id="cb133-25"><a href="07-glms.html#cb133-25"></a><span class="kw">plot</span>(bison<span class="op">$</span>d2f, bison<span class="op">$</span>occurrence, <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">col =</span> <span class="st">&#39;gray&#39;</span>,</span>
<span id="cb133-26"><a href="07-glms.html#cb133-26"></a>     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">6000</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb133-27"><a href="07-glms.html#cb133-27"></a>     <span class="dt">xlab=</span><span class="st">&#39;Distance to forest&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Bison occurrence&#39;</span>)</span>
<span id="cb133-28"><a href="07-glms.html#cb133-28"></a><span class="kw">points</span>(x_bar, y_bar, <span class="dt">pch =</span> <span class="dv">1</span>)</span>
<span id="cb133-29"><a href="07-glms.html#cb133-29"></a><span class="co"># add modelled mean response</span></span>
<span id="cb133-30"><a href="07-glms.html#cb133-30"></a><span class="kw">lines</span>(newdat<span class="op">$</span>d2f, y_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:bison"></span>
<img src="qm4g_files/figure-html/bison-1.png" alt="Left: Occurrence of bison as a function of forest distance with 1 indicating presence and 0 indicating absence. A logistic regression is fitted to these data that models the probability of occurrence. Note, due to the overlap of 1s and 0s to the left of the graph, the probability of occurrence comes out at only 0.6 for zero distance. Right: This can be better seen when plotting so called binned averages of the occurrence indicator against forest distance (open circles) after @gelman2020. Data from: [Benjamin Bleyhl](https://www.geographie.hu-berlin.de/en/professorships/biogeography/people/current-people/bleyhl)." width="50%" /><img src="qm4g_files/figure-html/bison-2.png" alt="Left: Occurrence of bison as a function of forest distance with 1 indicating presence and 0 indicating absence. A logistic regression is fitted to these data that models the probability of occurrence. Note, due to the overlap of 1s and 0s to the left of the graph, the probability of occurrence comes out at only 0.6 for zero distance. Right: This can be better seen when plotting so called binned averages of the occurrence indicator against forest distance (open circles) after @gelman2020. Data from: [Benjamin Bleyhl](https://www.geographie.hu-berlin.de/en/professorships/biogeography/people/current-people/bleyhl)." width="50%" />
<p class="caption">
Figure 6.8: Left: Occurrence of bison as a function of forest distance with 1 indicating presence and 0 indicating absence. A logistic regression is fitted to these data that models the probability of occurrence. Note, due to the overlap of 1s and 0s to the left of the graph, the probability of occurrence comes out at only 0.6 for zero distance. Right: This can be better seen when plotting so called binned averages of the occurrence indicator against forest distance (open circles) after <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>. Data from: <a href="https://www.geographie.hu-berlin.de/en/professorships/biogeography/people/current-people/bleyhl">Benjamin Bleyhl</a>.
</p>
</div>
</div>
<div id="goodness-of-fit-of-glms" class="section level2">
<h2><span class="header-section-number">6.5</span> Goodness of fit of GLMs</h2>
<p>Checking goodness of fit of GLMs works a little differently than for linear regression. Let us look at the output of the log-linear model fit to the birds data:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="07-glms.html#cb134-1"></a><span class="kw">summary</span>(birds_fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ relief, family = poisson(link = &quot;log&quot;), data = birds)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -11.198   -2.141   -0.002    2.199   10.720  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  4.66589    0.02158   216.2   &lt;2e-16 ***
## relief       0.33956    0.00838    40.5   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 2921.1  on 60  degrees of freedom
## Residual deviance: 1299.6  on 59  degrees of freedom
## AIC: 1740
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The coefficient information can be interpreted as for linear regression, but the <strong>coefficients</strong> themselves are given at the scale of the linear predictor (Figure <a href="07-glms.html#fig:loglin">6.5</a>, right), prior to its transformation through the inverse log-link, the exponential function, in this case. So we need to transform the linear predictor using the exponential function to see the effects of the coefficients on the original scale of the response (Figure <a href="07-glms.html#fig:loglin">6.5</a>, left). For the mean response we can write:
<span class="math display">\[y_i=\exp\left(4.67+0.34\cdot relief_i\right)=\exp(4.67)\cdot\exp(0.34\cdot relief_i)=107\cdot 1.4^{relief_i}\]</span>
So if <span class="math inline">\(relief_i=0\)</span> then <span class="math inline">\(y_i=107\)</span>, the intercept (bottom-left corner of Figure <a href="07-glms.html#fig:loglin">6.5</a>, left), and for every relief increment of 1 we <em>multiply</em> that intercept with 1.4. Hence the exponential increase of the mean response in Figure <a href="07-glms.html#fig:loglin">6.5</a>, left. For example, increasing relief by five increments (to 5) leads to a number of bird species <span class="math inline">\(y_i\)</span> of <span class="math inline">\(107\cdot 1.4^5=107\cdot 5.4=578\)</span> (top-right corner of Figure <a href="07-glms.html#fig:loglin">6.5</a>, left).</p>
<p>The <strong>residual deviance</strong> in this output requires more thought. It generalises the sum of squared errors (SSE) statistic of linear regression. In Chapter <a href="06-ml_bayes.html#mlbayes">5</a> we motivated minimising SSE by maximum likelihood theory and said that if residuals are normally distributed then minimising SSE yields the maximum likelihood estimate of the regression parameters. When using GLMs, when we do not make the normality assumption anymore, we cannot use SSE either as a goodness of fit measure. Instead we use residual deviance, which includes SSE as a special case when the response distribution is normal. The residual deviance is defined as:</p>
<p><span class="math display" id="eq:resdeviance">\[\begin{equation}
D=-2\cdot\left\{logL\left(\boldsymbol{\hat\theta}|\mathbf{y}\right)-logL\left(\boldsymbol{\hat\theta^{sat}}|\mathbf{y}\right)\right\}
\tag{6.13}
\end{equation}\]</span>
<span class="math inline">\(logL\left(\boldsymbol{\hat\theta}|\mathbf{y}\right)\)</span> is the log-likelihood of the maximum likelihood estimate of the model parameters, <span class="math inline">\(\boldsymbol{\hat\theta}\)</span>, which is compared by difference to the log-likelihood of the saturated model <span class="math inline">\(logL\left(\boldsymbol{\hat\theta^{sat}}|\mathbf{y}\right)\)</span>. Remember from multiple linear regression (Chapter <a href="05-multiple_lin_reg.html#multiplelinreg">4</a>) that the saturated model has one parameter per data point and hence gives a perfect fit.</p>
<p>Comparing two models by their log-likelihood difference amounts to comparing the models by their <strong>likelihood ratio</strong>:
<span class="math display" id="eq:lratio">\[\begin{equation}
D=-2\cdot\log\left\{\frac{L\left(\boldsymbol{\hat\theta}|\mathbf{y}\right)}{L\left(\boldsymbol{\hat\theta^{sat}}|\mathbf{y}\right)}\right\}
\tag{6.14}
\end{equation}\]</span>
The factor “-2” in front of the deviance is there to scale the log-likelihood difference so that it follows asymptotically a Chi-squared sampling distribution. See, for example, <span class="citation">McElreath (<a href="#ref-mcelreath2020" role="doc-biblioref">2020</a>)</span> and references therein; and compare our discussion of AIC in Chapter <a href="05-multiple_lin_reg.html#multiplelinreg">4</a>.</p>
<p>The <strong>deviance residuals</strong> summarised at the top of the output measure the contribution of each data point to the residual deviance, analogues to squared errors in linear regression.</p>
<p>The <strong>Null deviance</strong> is the log-likelihood difference of the Null model and the saturated model:
<span class="math display" id="eq:nulldeviance">\[\begin{equation}
D=-2\cdot\left\{logL\left(\boldsymbol{\hat\theta^0}|\mathbf{y}\right)-logL\left(\boldsymbol{\hat\theta^{sat}}|\mathbf{y}\right)\right\}
\tag{6.15}
\end{equation}\]</span>
Remember from Chapter <a href="05-multiple_lin_reg.html#multiplelinreg">4</a> that the Null model has the intercept only in the linear predictor, it is the overall mean on the scale of the linear predictor (prior to transformation via any inverse link-function).</p>
<p>We can then use the ratio of residual deviance to Null deviance as a goodness of fit measure analogues to the <span class="math inline">\(r^2\)</span> statistic of linear regression (compare Chapter <a href="03-lin_reg.html#linreg">2</a>); this is called <strong>pseudo-<span class="math inline">\(r^2\)</span></strong>:
<span class="math display" id="eq:pseudor2">\[\begin{equation}
\text{pseudo-}r^2=1-\frac{\text{residual deviance}}{\text{Null deviance}}
\tag{6.16}
\end{equation}\]</span>
For the birds example this is:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="07-glms.html#cb136-1"></a>pseudo_r2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>birds_fit<span class="op">$</span>deviance <span class="op">/</span><span class="st"> </span>birds_fit<span class="op">$</span>null.deviance</span>
<span id="cb136-2"><a href="07-glms.html#cb136-2"></a>pseudo_r2</span></code></pre></div>
<pre><code>## [1] 0.5551</code></pre>
<p>Finally, we need to look at the residuals as we did for linear regression, it is just that the raw residuals are structured by design for non-normal response distributions, hence they require standardisation. To understand how the raw residuals are structured by design, consider the Poisson model (Figure <a href="07-glms.html#fig:loglin">6.5</a>, left): Here the residual variance increases with the mean response (Figure <a href="07-glms.html#fig:rawres">6.9</a>, left), so seeing this structure in the residuals is expected and does not indicate a systematic error in the model. Consider further the binomial model (Figure <a href="07-glms.html#fig:logistic">6.7</a>, left): Here the residual variance peaks at 0.5 (Figure <a href="07-glms.html#fig:rawres">6.9</a>, right), which does not indicate a systematic error either.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="07-glms.html#cb138-1"></a><span class="co"># plot raw residuals of birds model</span></span>
<span id="cb138-2"><a href="07-glms.html#cb138-2"></a><span class="co"># the raw residuals are called with the type = &#39;response&#39; argument in resid()</span></span>
<span id="cb138-3"><a href="07-glms.html#cb138-3"></a><span class="kw">plot</span>(birds_fit<span class="op">$</span>fitted.values, <span class="kw">resid</span>(birds_fit, <span class="dt">type =</span> <span class="st">&#39;response&#39;</span>),</span>
<span id="cb138-4"><a href="07-glms.html#cb138-4"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">400</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">200</span>,<span class="dv">200</span>),</span>
<span id="cb138-5"><a href="07-glms.html#cb138-5"></a>     <span class="dt">xlab=</span><span class="st">&#39;Number of bird species&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residuals&#39;</span>)</span>
<span id="cb138-6"><a href="07-glms.html#cb138-6"></a><span class="co"># plot raw residuals of survival model</span></span>
<span id="cb138-7"><a href="07-glms.html#cb138-7"></a><span class="kw">plot</span>(survival_fit<span class="op">$</span>fitted.values, <span class="kw">resid</span>(survival_fit, <span class="dt">type =</span> <span class="st">&#39;response&#39;</span>),</span>
<span id="cb138-8"><a href="07-glms.html#cb138-8"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.15</span>),</span>
<span id="cb138-9"><a href="07-glms.html#cb138-9"></a>     <span class="dt">xlab=</span><span class="st">&#39;Proportion&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residuals&#39;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rawres"></span>
<img src="qm4g_files/figure-html/rawres-1.png" alt="Left: Raw residuals of the log-linear model fit to the birds data. The residual variance by design increases with increasing mean response. Right: Raw residuals of the logistic model fit to the survival data. The residual variance peaks at 0.5 by design." width="50%" /><img src="qm4g_files/figure-html/rawres-2.png" alt="Left: Raw residuals of the log-linear model fit to the birds data. The residual variance by design increases with increasing mean response. Right: Raw residuals of the logistic model fit to the survival data. The residual variance peaks at 0.5 by design." width="50%" />
<p class="caption">
Figure 6.9: Left: Raw residuals of the log-linear model fit to the birds data. The residual variance by design increases with increasing mean response. Right: Raw residuals of the logistic model fit to the survival data. The residual variance peaks at 0.5 by design.
</p>
</div>
<p>In order to correct for the change in residual variance that is built into these models, a useful standardisation of the raw residuals is by the standard deviation of the prediction at each data point. This gives the so called <strong>Pearson residuals</strong>:
<span class="math display" id="eq:pearsonres">\[\begin{equation}
\frac{y_i-\hat y_i}{\sqrt{Var\left(\hat y_i\right)}}
\tag{6.17}
\end{equation}\]</span></p>
<p>Depending on the formula for the standard deviation <span class="math inline">\(\sqrt{Var\left(\hat y_i\right)}\)</span>, the residuals take on different forms for the different response distributions (Table <a href="07-glms.html#tab:pearsonres">6.3</a>).</p>
<table>
<caption><span id="tab:pearsonres">Table 6.3: </span> Formula of the Pearson residuals for different response distributions, depending on their respective formula for the standard deviation.</caption>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Response distribution</th>
<th align="left">Standardised residuals (Pearson)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Normal</td>
<td align="left"><span class="math inline">\(\frac{y_i-\hat y_i}{\sigma}=\frac{y_i-\mu_i}{\sigma}\)</span></td>
</tr>
<tr class="even">
<td align="left">Poisson</td>
<td align="left"><span class="math inline">\(\frac{y_i-\hat y_i}{\sqrt{\hat y_i}}=\frac{y_i-\lambda_i}{\sqrt{\lambda_i}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Binomial</td>
<td align="left"><span class="math inline">\(\frac{y_i-\hat y_i}{\sqrt{\hat y_i\cdot\left[1-\frac{\hat y_i}{n_i}\right]}}=\frac{y_i-\pi_i\cdot n_i}{\sqrt{\pi_i\cdot n_i\cdot\left[1-\pi_i\right]}}\)</span></td>
</tr>
</tbody>
</table>
<p>For our examples, we plot the Pearson residuals in Figure <a href="07-glms.html#fig:pearsonres">6.10</a>. We see that some of the structure is removed, but not all, though this is hard to judge with so few data points.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="07-glms.html#cb139-1"></a><span class="co"># plot Pearson residuals of birds model</span></span>
<span id="cb139-2"><a href="07-glms.html#cb139-2"></a><span class="co"># this uses type = &#39;pearson&#39; in resid()</span></span>
<span id="cb139-3"><a href="07-glms.html#cb139-3"></a><span class="kw">plot</span>(birds_fit<span class="op">$</span>fitted.values, <span class="kw">resid</span>(birds_fit, <span class="dt">type =</span> <span class="st">&#39;pearson&#39;</span>),</span>
<span id="cb139-4"><a href="07-glms.html#cb139-4"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">400</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">15</span>,<span class="dv">15</span>),</span>
<span id="cb139-5"><a href="07-glms.html#cb139-5"></a>     <span class="dt">xlab=</span><span class="st">&#39;Number of bird species&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residuals&#39;</span>)</span>
<span id="cb139-6"><a href="07-glms.html#cb139-6"></a><span class="co"># plot Pearson residuals of survival model</span></span>
<span id="cb139-7"><a href="07-glms.html#cb139-7"></a><span class="kw">plot</span>(survival_fit<span class="op">$</span>fitted.values, <span class="kw">resid</span>(survival_fit, <span class="dt">type =</span> <span class="st">&#39;pearson&#39;</span>),</span>
<span id="cb139-8"><a href="07-glms.html#cb139-8"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>),</span>
<span id="cb139-9"><a href="07-glms.html#cb139-9"></a>     <span class="dt">xlab=</span><span class="st">&#39;Proportion&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residuals&#39;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:pearsonres"></span>
<img src="qm4g_files/figure-html/pearsonres-1.png" alt="Left: Pearson residuals of the log-linear model fit to the birds data. Compared to Figure \@ref(fig:rawres), left, the increase in residual variance with increasing mean response is dampened. Right: Pearson residuals of the logistic model fit to the survival data. Compared to Figure \@ref(fig:rawres), right, the peak in residual variance at 0.5 is attenuated." width="50%" /><img src="qm4g_files/figure-html/pearsonres-2.png" alt="Left: Pearson residuals of the log-linear model fit to the birds data. Compared to Figure \@ref(fig:rawres), left, the increase in residual variance with increasing mean response is dampened. Right: Pearson residuals of the logistic model fit to the survival data. Compared to Figure \@ref(fig:rawres), right, the peak in residual variance at 0.5 is attenuated." width="50%" />
<p class="caption">
Figure 6.10: Left: Pearson residuals of the log-linear model fit to the birds data. Compared to Figure <a href="07-glms.html#fig:rawres">6.9</a>, left, the increase in residual variance with increasing mean response is dampened. Right: Pearson residuals of the logistic model fit to the survival data. Compared to Figure <a href="07-glms.html#fig:rawres">6.9</a>, right, the peak in residual variance at 0.5 is attenuated.
</p>
</div>
</div>
<div id="overdisp" class="section level2">
<h2><span class="header-section-number">6.6</span> Over-dispersion</h2>
<p>The residuals of the log-linear model of the birds data still show a pattern; after standardisation they still increase with increasing mean response (Figure <a href="07-glms.html#fig:pearsonres">6.10</a>, left). Remember that in the Poisson model the variance is modelled to increase 1:1 with the mean response (Figure <a href="07-glms.html#fig:birds">6.6</a>). If we see the actual variance in our birds data increasing at a greater rate, then we say the data are <strong>over-dispersed</strong>.</p>
<p>In such cases we might use the so called <strong>negative-binomial model</strong> which has two parameters: <span class="math inline">\(\mu\)</span>, which is equal to the mean, and <span class="math inline">\(\theta\)</span>, which modulates the variance in the form of <span class="math inline">\(\mu+\frac{\mu^2}{\theta}\)</span>, i.e. the variance is now a non-linear function of the mean, and not equal to the mean as in the Poisson model. This allows for over-dispersion.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> The negative-binomial is available through the <code>MASS</code> package in <em>R</em>. Let us see how it fits the birds data:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="07-glms.html#cb140-1"></a><span class="kw">library</span>(<span class="st">&quot;MASS&quot;</span>)</span>
<span id="cb140-2"><a href="07-glms.html#cb140-2"></a><span class="co"># fit negative-binomial model with log-link to birds data</span></span>
<span id="cb140-3"><a href="07-glms.html#cb140-3"></a>birds_fit2 &lt;-<span class="st"> </span><span class="kw">glm.nb</span>(y <span class="op">~</span><span class="st"> </span>relief, <span class="dt">link =</span> log, <span class="dt">data =</span> birds)</span>
<span id="cb140-4"><a href="07-glms.html#cb140-4"></a><span class="co"># summarise regression</span></span>
<span id="cb140-5"><a href="07-glms.html#cb140-5"></a><span class="kw">summary</span>(birds_fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm.nb(formula = y ~ relief, data = birds, link = log, init.theta = 11.71533906)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3174  -0.5935  -0.0551   0.4673   2.1245  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   4.7245     0.0880   53.71  &lt; 2e-16 ***
## relief        0.3127     0.0389    8.03  9.6e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Negative Binomial(11.72) family taken to be 1)
## 
##     Null deviance: 134.313  on 60  degrees of freedom
## Residual deviance:  61.519  on 59  degrees of freedom
## AIC: 681.9
## 
## Number of Fisher Scoring iterations: 1
## 
## 
##               Theta:  11.72 
##           Std. Err.:  2.21 
## 
##  2 x log-likelihood:  -675.88</code></pre>
<p>The parameters of the linear predictor are very similar to those of the Poisson model. Hence the modelled mean response looks very similar:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="07-glms.html#cb142-1"></a><span class="co"># plot negative-binomial fit</span></span>
<span id="cb142-2"><a href="07-glms.html#cb142-2"></a><span class="kw">plot</span>(birds<span class="op">$</span>relief, birds<span class="op">$</span>y, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">600</span>),</span>
<span id="cb142-3"><a href="07-glms.html#cb142-3"></a>     <span class="dt">xlab=</span><span class="st">&#39;Average relief&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Number of bird species&#39;</span>)</span>
<span id="cb142-4"><a href="07-glms.html#cb142-4"></a>newdat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">relief=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="fl">0.01</span>))</span>
<span id="cb142-5"><a href="07-glms.html#cb142-5"></a>y_new &lt;-<span class="st"> </span><span class="kw">predict</span>(birds_fit2, <span class="dt">newdata=</span>newdat, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb142-6"><a href="07-glms.html#cb142-6"></a><span class="kw">lines</span>(newdat<span class="op">$</span>relief, y_new, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-34-1.png" width="80%" /></p>
<p>But now we have the additional dispersion parameter <span class="math inline">\(\theta\)</span>, which comes out at 11.72 for the birds data, which leads to a better coverage of the response distribution compared to the Poisson model:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="07-glms.html#cb143-1"></a><span class="co"># plot overall response distribution for negative-binomial fit</span></span>
<span id="cb143-2"><a href="07-glms.html#cb143-2"></a><span class="kw">plot</span>(h0, <span class="dt">freq =</span> <span class="ot">FALSE</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.02</span>),</span>
<span id="cb143-3"><a href="07-glms.html#cb143-3"></a>     <span class="dt">main =</span> <span class="st">&quot;overall negative-binomial model&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Number of bird species&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;pdf&quot;</span>)</span>
<span id="cb143-4"><a href="07-glms.html#cb143-4"></a><span class="kw">curve</span>((</span>
<span id="cb143-5"><a href="07-glms.html#cb143-5"></a>        <span class="kw">dnbinom</span>(x, <span class="dt">mu=</span><span class="kw">predict</span>(birds_fit2, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">relief=</span><span class="fl">0.5760</span>),</span>
<span id="cb143-6"><a href="07-glms.html#cb143-6"></a>                              <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>), <span class="dt">size=</span>birds_fit2<span class="op">$</span>theta) <span class="op">+</span></span>
<span id="cb143-7"><a href="07-glms.html#cb143-7"></a><span class="st">                </span><span class="kw">dnbinom</span>(x, <span class="dt">mu=</span><span class="kw">predict</span>(birds_fit2, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">relief=</span><span class="fl">1.5690</span>),</span>
<span id="cb143-8"><a href="07-glms.html#cb143-8"></a>                                      <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>), <span class="dt">size=</span>birds_fit2<span class="op">$</span>theta) <span class="op">+</span></span>
<span id="cb143-9"><a href="07-glms.html#cb143-9"></a><span class="st">                </span><span class="kw">dnbinom</span>(x, <span class="dt">mu=</span><span class="kw">predict</span>(birds_fit2, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">relief=</span><span class="fl">2.4510</span>),</span>
<span id="cb143-10"><a href="07-glms.html#cb143-10"></a>                                      <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>), <span class="dt">size=</span>birds_fit2<span class="op">$</span>theta) <span class="op">+</span></span>
<span id="cb143-11"><a href="07-glms.html#cb143-11"></a><span class="st">                </span><span class="kw">dnbinom</span>(x, <span class="dt">mu=</span><span class="kw">predict</span>(birds_fit2, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">relief=</span><span class="fl">3.6090</span>),</span>
<span id="cb143-12"><a href="07-glms.html#cb143-12"></a>                                      <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>), <span class="dt">size=</span>birds_fit2<span class="op">$</span>theta))<span class="op">/</span><span class="dv">4</span>,</span>
<span id="cb143-13"><a href="07-glms.html#cb143-13"></a>      <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">type =</span> <span class="st">&#39;b&#39;</span>)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-35-1.png" width="80%" /></p>
<p>AIC - indicative of predictive performance - is a lot better compared to the Poisson model, even if Pseudo-<span class="math inline">\(r^2\)</span> is a little worse:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="07-glms.html#cb144-1"></a>birds_fit2<span class="op">$</span>aic</span></code></pre></div>
<pre><code>## [1] 681.9</code></pre>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="07-glms.html#cb146-1"></a>pseudo_r2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>birds_fit2<span class="op">$</span>deviance <span class="op">/</span><span class="st"> </span>birds_fit2<span class="op">$</span>null.deviance</span>
<span id="cb146-2"><a href="07-glms.html#cb146-2"></a>pseudo_r2</span></code></pre></div>
<pre><code>## [1] 0.542</code></pre>
<p>The residuals show some improvement, but not much:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="07-glms.html#cb148-1"></a><span class="co"># plot Pearson residuals of negative-binomial fit</span></span>
<span id="cb148-2"><a href="07-glms.html#cb148-2"></a><span class="kw">plot</span>(birds_fit2<span class="op">$</span>fitted.values, <span class="kw">resid</span>(birds_fit2, <span class="dt">type =</span> <span class="st">&#39;pearson&#39;</span>),</span>
<span id="cb148-3"><a href="07-glms.html#cb148-3"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">400</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>),</span>
<span id="cb148-4"><a href="07-glms.html#cb148-4"></a>     <span class="dt">xlab=</span><span class="st">&#39;Number of bird species&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residuals&#39;</span>)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-37-1.png" width="80%" /></p>
<p>It seems we have reached the limit here of what we can achieve with just a single predictor. Further improvements should only be expected if we introduce additional predictors of the number of bird species.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-crawley2012">
<p>Crawley, M. J. 2012. <em>The R Book (2nd Ed.)</em>. Chichester: John Wiley &amp; Sons Ltd.</p>
</div>
<div id="ref-dormann2013">
<p>Dormann, C. F. 2013. <em>Parametrische Statistik</em>. Berlin: Springer.</p>
</div>
<div id="ref-gelman2020">
<p>Gelman, A., J. Hill, and A. Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-mcelreath2020">
<p>McElreath, R. 2020. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd Ed.)</em>. Boca Raton: CRC Press.</p>
</div>
<div id="ref-piegorsch2005">
<p>Piegorsch, W. W., and A. J. Bailer. 2005. <em>Analyzing Environmental Data</em>. Chichester: John Wiley &amp; Sons, Ltd.</p>
</div>
<div id="ref-quinn2002">
<p>Quinn, G. P., and M. J. Keough. 2002. <em>Experimental Design and Data Analysis for Biologists</em>. Cambridge: Cambridge University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p><span class="math inline">\(E\left[\cdot\right]\)</span> stands for <strong>expectation</strong>, the mean of an entity, here the response variable <span class="math inline">\(y_i\)</span>, which - if the entity is normally distributed - is the parameter <span class="math inline">\(\mu_i\)</span>.<a href="07-glms.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>For <strong>relief</strong> they use a measure of the difference between the lowest and the highest elevation in study area.<a href="07-glms.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>In <em>Applied Statistical Modelling</em> we show that the Bayesian statistical framework has a unifying framework for solving regression problems via sampling, which is conceptually simple but can be computationally demanding.<a href="07-glms.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>The <strong>factorial</strong> of a whole number is the product of all smaller whole numbers up to that number, e.g. <span class="math inline">\(5!=5\cdot 4\cdot 3\cdot 2\cdot 1\)</span>.<a href="07-glms.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>The <strong>binomial coefficient</strong> <span class="math inline">\(\binom{n}{y}\)</span> gives the number of ways, disregarding order, that we can choose <span class="math inline">\(y\)</span> out of <span class="math inline">\(n\)</span>.<a href="07-glms.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>You might know <strong>odds</strong> from betting: If <span class="math inline">\(\pi=0.8\)</span> is the chance of a favourable outcome, e.g. our favourite horse winning, then <span class="math inline">\(\frac{\pi}{1-\pi}=\frac{0.8}{0.2}=4\)</span> are the odds of winning, “4 agains 1”. In our survival example it is the odds of surviving.<a href="07-glms.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>Indeed, proportion data can be interpreted as an aggregation of a number of “trials” with a binary response. In the survival example: The proportion of organisms surviving at each concentration level is just the average of the survival indicator for all <span class="math inline">\(n\)</span> organisms with 1=surviving and 0=not surviving.<a href="07-glms.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>Note, there are other over-dispersed model choices.<a href="07-glms.html#fnref22" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="06-ml_bayes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="08-multivariate.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
