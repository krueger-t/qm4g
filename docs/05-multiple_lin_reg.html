<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Multiple linear regression | Quantitative Methods for Geographers</title>
  <meta name="description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Multiple linear regression | Quantitative Methods for Geographers" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="github-repo" content="krueger-t/qm4g" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Multiple linear regression | Quantitative Methods for Geographers" />
  
  <meta name="twitter:description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  

<meta name="author" content="Tobias Krueger" />


<meta name="date" content="2021-11-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="04-categorical_vars.html"/>
<link rel="next" href="06-ml_bayes.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block/empty-anchor.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding/rglWebGL.js"></script>
<link href="libs/rglwidgetClass/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass/rglClass.src.js"></script>
<script src="libs/rglwidgetClass/utils.src.js"></script>
<script src="libs/rglwidgetClass/subscenes.src.js"></script>
<script src="libs/rglwidgetClass/shaders.src.js"></script>
<script src="libs/rglwidgetClass/textures.src.js"></script>
<script src="libs/rglwidgetClass/projection.src.js"></script>
<script src="libs/rglwidgetClass/mouse.src.js"></script>
<script src="libs/rglwidgetClass/init.src.js"></script>
<script src="libs/rglwidgetClass/pieces.src.js"></script>
<script src="libs/rglwidgetClass/draw.src.js"></script>
<script src="libs/rglwidgetClass/controls.src.js"></script>
<script src="libs/rglwidgetClass/selection.src.js"></script>
<script src="libs/rglwidgetClass/rglTimer.src.js"></script>
<script src="libs/CanvasMatrix4/CanvasMatrix.src.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />
<link href="libs/bsTable/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable/bootstrapTable.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Geographers</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="02-math.html"><a href="02-math.html"><i class="fa fa-check"></i><b>1</b> Mathematical preliminaries</a></li>
<li class="chapter" data-level="2" data-path="03-lin_reg.html"><a href="03-lin_reg.html"><i class="fa fa-check"></i><b>2</b> Linear regression</a></li>
<li class="chapter" data-level="3" data-path="04-categorical_vars.html"><a href="04-categorical_vars.html"><i class="fa fa-check"></i><b>3</b> Categorical predictors</a></li>
<li class="chapter" data-level="4" data-path="05-multiple_lin_reg.html"><a href="05-multiple_lin_reg.html"><i class="fa fa-check"></i><b>4</b> Multiple linear regression</a></li>
<li class="chapter" data-level="5" data-path="06-ml_bayes.html"><a href="06-ml_bayes.html"><i class="fa fa-check"></i><b>5</b> Probabilistic underpinnings</a></li>
<li class="chapter" data-level="6" data-path="07-glms.html"><a href="07-glms.html"><i class="fa fa-check"></i><b>6</b> Generalised Linear Models (GLMs)</a></li>
<li class="chapter" data-level="7" data-path="08-multivariate.html"><a href="08-multivariate.html"><i class="fa fa-check"></i><b>7</b> Multivariate methods</a></li>
<li class="chapter" data-level="" data-path="09-solutions.html"><a href="09-solutions.html"><i class="fa fa-check"></i>Solutions to exercises</a></li>
<li class="chapter" data-level="" data-path="10-refs.html"><a href="10-refs.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Geographers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiplelinreg" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Multiple linear regression</h1>
<p>The extension of linear regression to the case of <strong>more than one predictor</strong> - be they continuous or categorical or a mix of both - is called multiple linear regression. This means we go from the equation <span class="math inline">\(y=\beta_0+\beta_1\cdot x+\epsilon\)</span> (Equation <a href="03-lin_reg.html#eq:linmodsingle">(2.2)</a>) to the equation <span class="math inline">\(y=\beta_0+\sum_{j=1}^{p}\beta_j\cdot x_j+\epsilon\)</span> (Equation <a href="03-lin_reg.html#eq:linmod">(2.1)</a>).</p>
<p>As an example we will look at a dataset of air quality, again from <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span> - see Figure <a href="05-multiple_lin_reg.html#fig:ozonematrix">4.1</a> - asking the question: <em>How is ground-level ozone concentration related to wind speed, air temperature and solar radiation intensity?</em></p>
<p>A useful first thing to do is to plot what’s often called a <strong>scatterplot matrix</strong> (Figure <a href="05-multiple_lin_reg.html#fig:ozonematrix">4.1</a>).</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="05-multiple_lin_reg.html#cb58-1"></a><span class="co"># load air quality data</span></span>
<span id="cb58-2"><a href="05-multiple_lin_reg.html#cb58-2"></a>ozone &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/ozone.txt&quot;</span>,<span class="dt">header=</span>T)</span></code></pre></div>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="05-multiple_lin_reg.html#cb59-1"></a><span class="co"># scatterplot matrix of ozone dataset</span></span>
<span id="cb59-2"><a href="05-multiple_lin_reg.html#cb59-2"></a><span class="co"># this requires running example(pairs) first so that the histogramms can be drawn on the diagonal</span></span>
<span id="cb59-3"><a href="05-multiple_lin_reg.html#cb59-3"></a><span class="co"># here this is done in the background</span></span>
<span id="cb59-4"><a href="05-multiple_lin_reg.html#cb59-4"></a><span class="kw">pairs</span>(ozone, <span class="dt">diag.panel =</span> panel.hist, <span class="dt">lower.panel =</span> panel.cor)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ozonematrix"></span>
<img src="qm4g_files/figure-html/ozonematrix-1.png" alt="Scatterplot matrix of ozone dataset: rad = solar radiation intensity; temp = air temperature; wind = wind speed; ozone = ground-level ozone concentration. The diagonal shows the histograms of the individual variables. The lower triangle shows the linear correlation coefficients, with font size proportional to size of correlation. Data from: @crawley2012" width="80%" />
<p class="caption">
Figure 4.1: Scatterplot matrix of ozone dataset: rad = solar radiation intensity; temp = air temperature; wind = wind speed; ozone = ground-level ozone concentration. The diagonal shows the histograms of the individual variables. The lower triangle shows the linear correlation coefficients, with font size proportional to size of correlation. Data from: <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span>
</p>
</div>
<p>On the diagonal you see the histograms of each variable. On the upper triangle you see scatterplots between two variables. On the lower triangle you see linear correlation coefficients, with font size proportional to size of correlation. From this we already see that “ozone” is correlated with all three other variables, but perhaps less so with “radiation”. We also see that the other variables are correlated, at least “wind” and “temperature”.</p>
<p>The challenge we now face is typical for multiple regression - it is one of model selection:</p>
<ul>
<li>Which predictor variables to include? E.g. possibly <span class="math inline">\(x_1=rad\)</span>, <span class="math inline">\(x_2=temp\)</span>, <span class="math inline">\(x_3=wind\)</span></li>
<li>Which interactions between variables to include? E.g. possibly <span class="math inline">\(x_4=rad\cdot temp\)</span>, <span class="math inline">\(x_5=rad\cdot wind\)</span>, <span class="math inline">\(x_6=temp\cdot wind\)</span>, <span class="math inline">\(x_7=rad\cdot temp\cdot wind\)</span></li>
</ul>
<p>We have not talked about <strong>interactions</strong> so far - because we had just one predictor variable to think about - but interactions are quite an interesting element of regression models. Essentially, one predictor could modulate the effect that another predictor has on the response - we will discuss an example below in Chapter <a href="05-multiple_lin_reg.html#ancova">4.5</a>. Mathematically, this is achieved by adding the product of the two predictors as an extra predictor to the linear model, on top of the two individual predictors. In addition to such <em>2-way interactions</em>, we can have <em>3-way interactions</em> (three predictors multiplied) and so on and so forth, but these get increasingly complicated to interpret.</p>
<p>In principle, we could also include <strong>higher-order terms</strong>, such as <span class="math inline">\(x_8=rad^2\)</span>, <span class="math inline">\(x_9=temp^2\)</span>, <span class="math inline">\(x_{10}=wind^2\)</span> etc., in a regression model, as well as other predictor transformations. In practice, though, such choices will only be included if there is a theoretical reason to include them.</p>
<p>We then face two main problems in model selection:</p>
<ul>
<li><strong>Collinearity of variables</strong>: Predictors may be correlated with each other, which complicates their estimation. Interactions (and higher-order terms) certainly introduce collinearity as we will see below.</li>
<li><strong>Overfitting</strong>: The more predictors (and hence parameters) we add the better we can fit the data; but with an increasing risk of fitting the noise and not just the signal in the data, which will lead to poor predictions.</li>
</ul>
<p>We discuss these points in turn in the next section.</p>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">4.1</span> Model selection</h2>
<p>Model selection often appeals to the <strong>Parsimony Principle</strong> or <strong>Occam’s Razor</strong>. It goes roughly like this: Given a set of models with “similar explanatory power”, the “simplest” of these shall be preferred. This is called a “philosophical razor”, i.e. a rule of thumb that narrows down the choices of possible explanation or action. It dates back to the English Franciscan friar William of Ockham (also Occam; c. 1287-1347). His was a time of religious controversy and William of Ockham was one of those who advocated for a “simple” life (and by extension a poor and not a rich clergy, which got him into trouble).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> I believe this quest for simplicity carried over to his view on epistemology (how we know things) - hence Occam’s Razor. Anyhow, for us in statistical modelling Occam’s Razor roughly translates to the following guidelines (after <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span>):</p>
<ul>
<li>Prefer a model with <span class="math inline">\(m-1\)</span> parameters to a model with <span class="math inline">\(m\)</span> parameters</li>
<li>Prefer a model with <span class="math inline">\(k-1\)</span> explanatory variables to a model with <span class="math inline">\(k\)</span> explanatory variables</li>
<li>Prefer a linear model to a non-linear model</li>
<li>Prefer a model without interactions to a model containing interactions between explanatory variables</li>
<li>A model shall be simplified until it is <strong>minimal adequate</strong></li>
</ul>
<p>To understand “minimal adequate”, consider Table <a href="05-multiple_lin_reg.html#tab:minadequat">4.1</a>.</p>
<table>
<caption><span id="tab:minadequat">Table 4.1: </span> Model complexity types in model selection. After: <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span>.</caption>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Saturated model</th>
<th align="center">Maximal model</th>
<th align="center">Minimal adequate model</th>
<th align="center">Null model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">One parameter for every data point</td>
<td align="center">Contains all (<span class="math inline">\(p\)</span>) explanatory variables and interactions that might be of interest (many likely insignificant)</td>
<td align="center">Simplified model with <span class="math inline">\(p&#39;\)</span> parameters (<span class="math inline">\(0\leq p&#39;\leq p\)</span>)</td>
<td align="center">Just one parameter, the overall mean <span class="math inline">\(\bar y\)</span></td>
</tr>
<tr class="even">
<td align="center">Fit: perfect</td>
<td align="center">Fit: less than perfect</td>
<td align="center">Fit: less than maximal model, but not significantly so</td>
<td align="center">Fit: none, <span class="math inline">\(SSE=SSY\)</span></td>
</tr>
<tr class="odd">
<td align="center">Degrees of freedom: <span class="math inline">\(0\)</span></td>
<td align="center">Degrees of freedom: <span class="math inline">\(n-p-1\)</span></td>
<td align="center">Degrees of freedom: <span class="math inline">\(n-p&#39;-1\)</span></td>
<td align="center">Degrees of freedom: <span class="math inline">\(n-1\)</span></td>
</tr>
<tr class="even">
<td align="center">Explanatory power: none</td>
<td align="center">Explanatory power: <span class="math inline">\(r^2=1-\frac{SSE}{SSY}\)</span></td>
<td align="center">Explanatory power: <span class="math inline">\(r^2=1-\frac{SSE}{SSY}\)</span></td>
<td align="center">Explanatory power: none</td>
</tr>
</tbody>
</table>
<p>At one end of the complexity spectrum of potential models is the so called <strong>saturated model</strong>. It has one parameter for every data point and hence fits the data perfectly - this can be shown mathematically and is displayed in Figure <a href="05-multiple_lin_reg.html#fig:poly">4.2</a> for a polynomial of <span class="math inline">\((n-1)\)</span>th order. But this model has zero degrees of freedom, hence has no explanatory power; it fits the noise around the signal perfectly, which has no use in prediction - just imagine to use the <span class="math inline">\((n-1)\)</span>th-order polynomial (or even lower-order ones) in Figure <a href="05-multiple_lin_reg.html#fig:poly">4.2</a> for extrapolation.</p>
<div class="figure" style="text-align: center"><span id="fig:poly"></span>
<img src="figs/poly.jpg" alt="A dataset of nine data points is fitted by polynomials of varying order; poly1=1st-order to poly8=8th-order. The 8th-order polynomial (equation at top of figure) fits the data perfectly as it is a saturated model; it has as many parameters as data points. The Null model (intercept only) is just the mean of $y$; equation at bottom right." width="80%" />
<p class="caption">
Figure 4.2: A dataset of nine data points is fitted by polynomials of varying order; poly1=1st-order to poly8=8th-order. The 8th-order polynomial (equation at top of figure) fits the data perfectly as it is a saturated model; it has as many parameters as data points. The Null model (intercept only) is just the mean of <span class="math inline">\(y\)</span>; equation at bottom right.
</p>
</div>
<p>At the other end of the complexity spectrum is what is called the <strong>Null model</strong>. Its only parameter is the intercept <span class="math inline">\(\beta_0\)</span>, whose best estimate minimising the sum of squared errors <span class="math inline">\(SSE\)</span> is the mean of <span class="math inline">\(y\)</span>, i.e. <span class="math inline">\(\bar y\)</span> (see also Figure <a href="05-multiple_lin_reg.html#fig:poly">4.2</a>). The Null model does not fit anything beyond <span class="math inline">\(SSY\)</span>, the variation around the mean, hence does not explain any relations in the data.</p>
<p>In between those polar opposites are the so called maximal model and the minimal adequate model. These terms are only loosely defined but mark the space of model complexity that we navigate in model selection. The <strong>maximal model</strong> contains all explanatory variables and interactions that might be of interest, of which many will likely turn out insignificant. It fits the data less than perfect - but that is not the goal anyway given noise - and its explanatory power can be judged with <span class="math inline">\(r^2\)</span>, for example. I suggest that the maximal model be strongly informed by our underlying (theoretical) understanding of the relations to be modelled, and that predictors and interactions that do not make any sense in relation to that understanding be excluded. This approach, of course, will limit our exposure to surprises, which we could learn a lot from, but aims at keeping the model selection problem manageable.</p>
<p>The <strong>minimal adequate model</strong> then includes the subset of predictors of the maximal model that “really matter”, naturally compromising some goodness of fit of the maximal model, but not significantly so - that is the trick. What “really matters” in that sense depends. Under the classic statistical paradigm, this has a lot to do with <em>statistical significance</em> (the p-values of the parameter estimates); we rarely include parameters that are insignificant.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> But since anything can be significant with enough data points, the cutoff at a significance level of say <span class="math inline">\(\alpha=0.01\)</span> seems arbitrary. But p-values can be taken as functions of the standard errors of the parameter estimates - this is simply what they are - and this is what they are useful for. In general we do not want too many parameters in our models because this inflates their standard errors, making their interpretation essentially meaningless - so looking at standard errors (via p-values if you must) is important. But we want to be able to include the occasional parameter that has a large standard error (and is insignificant in a classic sense), simply because there might be a mechanistic reason to do so, especially for prediction. The standard error of that parameter may be large because there is little information in the data at hand about the parameter, but that is no reason to exclude it if we have reason to believe it is important. In this case the large standard error just helps to be honest about the capabilities of our model given the data at hand. So, in sum, let’s not be overly concerned about p-values during model selection. Instead, as we will see below (Chapter <a href="05-multiple_lin_reg.html#aic">4.4</a>), it is a good idea to base model selection on information criteria as these approximate the models <em>predictive</em> performance.</p>
<p>Model selection involves a lot of trial and error and personal judgement, but there are a few guidelines. In general, we can follow an up-ward or a down-ward selection of models. <strong>Up-ward</strong> model selection starts with a minimal set of predictors and sequentially adds more when this increases some information criterion or other measure. <strong>Down-ward</strong> model selection starts with the maximal model and sequentially simplifies this. Along each way there will be steps where we test several models - different routes to take for complication or simplification, respectively. Again, information criteria are crucial here. We can do this manually but automatic model selection algorithms are available in <em>R</em> - you will learn some of them in the PC-lab. But I would not trust them blindly - always confirm the result by testing the final model against some alternatives.</p>
</div>
<div id="collinearity" class="section level2">
<h2><span class="header-section-number">4.2</span> Collinearity</h2>
<p>Predictors are collinear (sometimes called multicollinear) when they are perfectly correlated, i.e. when a linear combination of them exists that equals zero for all the data (the estimated parameter values can compensate each other). The parameters then cannot be estimated uniquely (the estimates have standard errors of infinity) - they are said to be <strong>nonidentifiable</strong>.</p>
<p>In practice, predictors are seldom perfectly correlated, so <em>near-collinearity</em> and <em>poor identifiability</em> are the issues to worry about. In the ozone dataset, we see weak collinearity between predictors, so nothing to worry about too much at this stage (Figure <a href="05-multiple_lin_reg.html#fig:ozonematrix">4.1</a>). Modelling interactions between predictors, however, introduces collinearity (Figure <a href="05-multiple_lin_reg.html#fig:aqinteract">4.3</a>).</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="05-multiple_lin_reg.html#cb60-1"></a><span class="co"># generate new dataset w interactions</span></span>
<span id="cb60-2"><a href="05-multiple_lin_reg.html#cb60-2"></a>ozone2 &lt;-<span class="st"> </span>ozone</span>
<span id="cb60-3"><a href="05-multiple_lin_reg.html#cb60-3"></a>ozone2<span class="op">$</span>rad_temp &lt;-<span class="st"> </span>ozone<span class="op">$</span>rad <span class="op">*</span><span class="st"> </span>ozone<span class="op">$</span>temp</span>
<span id="cb60-4"><a href="05-multiple_lin_reg.html#cb60-4"></a>ozone2<span class="op">$</span>rad_wind &lt;-<span class="st"> </span>ozone<span class="op">$</span>rad <span class="op">*</span><span class="st"> </span>ozone<span class="op">$</span>wind</span>
<span id="cb60-5"><a href="05-multiple_lin_reg.html#cb60-5"></a>ozone2<span class="op">$</span>temp_wind &lt;-<span class="st"> </span>ozone<span class="op">$</span>temp <span class="op">*</span><span class="st"> </span>ozone<span class="op">$</span>wind</span>
<span id="cb60-6"><a href="05-multiple_lin_reg.html#cb60-6"></a>ozone2<span class="op">$</span>rad_temp_wind &lt;-<span class="st"> </span>ozone<span class="op">$</span>rad <span class="op">*</span><span class="st"> </span>ozone<span class="op">$</span>temp <span class="op">*</span><span class="st"> </span>ozone<span class="op">$</span>wind</span></code></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="05-multiple_lin_reg.html#cb61-1"></a><span class="kw">pairs</span>(ozone2, <span class="dt">diag.panel =</span> panel.hist, <span class="dt">lower.panel =</span> panel.cor)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:aqinteract"></span>
<img src="qm4g_files/figure-html/aqinteract-1.png" alt="Scatterplot matrix of ozone dataset, including interactions. Interaction terms are generally correlated with the predictors interacting. Data from: @crawley2012" width="80%" />
<p class="caption">
Figure 4.3: Scatterplot matrix of ozone dataset, including interactions. Interaction terms are generally correlated with the predictors interacting. Data from: <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span>
</p>
</div>
<p>We can live with mild levels of collinearity, for reasons discussed above, especially if including interactions, for example, is important for mechanistic reasons. However, if standard errors become so large as to make the parameter estimates essentially meaningless, we need to leave out some of the correlated predictors - even if we find them important from a mechanistic perspective. Another option is to transform the predictors by <em>Principal Component Analysis</em> (PCA; Chapter <a href="08-multivariate.html#multivariate">7</a>) into a set of new, uncorrelated predictors that combine the information of the original predictors.</p>
</div>
<div id="overfitting" class="section level2">
<h2><span class="header-section-number">4.3</span> Overfitting</h2>
<p>Overfitting occurs when we try to estimate too many parameters compared to the size of the dataset at hand. Then we will unduly fit the noise around the signal that interests us, from which there is nothing to be learned. We will also inflate standard errors as more and more parameters become less and less identifiable. This is illustrated with polynomials in Figure <a href="05-multiple_lin_reg.html#fig:poly">4.2</a>.</p>
<p>To get an idea of how common this problem is we can look at another air quality dataset, also from <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span> (Figure <a href="05-multiple_lin_reg.html#fig:sulphurmatrix">4.4</a>).</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="05-multiple_lin_reg.html#cb62-1"></a><span class="co"># load 2nd air quality data</span></span>
<span id="cb62-2"><a href="05-multiple_lin_reg.html#cb62-2"></a>sulphur &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/sulphur.txt&quot;</span>,<span class="dt">header=</span>T)</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="05-multiple_lin_reg.html#cb63-1"></a><span class="kw">pairs</span>(sulphur, <span class="dt">diag.panel =</span> panel.hist, <span class="dt">lower.panel =</span> panel.cor)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sulphurmatrix"></span>
<img src="qm4g_files/figure-html/sulphurmatrix-1.png" alt="Scatterplot matrix of sulphur dataset: Pollution = sulphur dioxide concentration; Temp = air temperature; Industry = prevalence of industry; Population = population size; Wind = wind speed; Rain = rainfall; Wet-days = number of wet days. Data from: @crawley2012" width="80%" />
<p class="caption">
Figure 4.4: Scatterplot matrix of sulphur dataset: Pollution = sulphur dioxide concentration; Temp = air temperature; Industry = prevalence of industry; Population = population size; Wind = wind speed; Rain = rainfall; Wet-days = number of wet days. Data from: <span class="citation">Crawley (<a href="#ref-crawley2012" role="doc-biblioref">2012</a>)</span>
</p>
</div>
<p>Here we have 41 data points and six possible main predictors. How many possible predictors can we have by including all possible interactions? (Q2)<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Once you’ve worked this out you see that we have to be really selective if we want to include interactions, because we approach the saturated model (Figure <a href="05-multiple_lin_reg.html#fig:poly">4.2</a>) very quickly.</p>
</div>
<div id="aic" class="section level2">
<h2><span class="header-section-number">4.4</span> Information criteria</h2>
<p>Information criteria help us select models because they approximate the models’ <em>predictive performance</em>, i.e. how well they would fit observations not included when fitting the models (“out-of-sample”). Information criteria penalise models for overfitting because overfitting makes predictive performance worse. Under the classical statistical paradigm, the preferred information criterion is arguably the <strong>Akaike Information Criterion</strong> (AIC):<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
<span class="math display" id="eq:aic">\[\begin{equation}
AIC=-2\cdot logL\left(\hat\beta, \hat\sigma|\mathbf{y}\right)+2\cdot p
\tag{4.1}
\end{equation}\]</span>
<span class="math inline">\(logL\left(\hat \beta, \hat \sigma|\mathbf{y}\right)\)</span> is the log-likelihood function at the maximum likelihood estimate of the parameters <span class="math inline">\(\hat \beta\)</span>, with <span class="math inline">\(\hat \sigma=\sqrt{\frac{SSE}{n-p}}\)</span>, given the data <span class="math inline">\(\mathbf{y}\)</span>. The maximum likelihood estimate is the Least Squares estimate for linear regression. We will learn more about the likelihood function and its relation to Least Squares in Chapter <a href="06-ml_bayes.html#mlbayes">5</a>. <span class="math inline">\(p\)</span> is the number of parameters in our model. Under the classic statistical paradigm, AIC is a reliable approximation of out-of-sample performance only when the likelihood function is approximately normal (which is a standard assumption anyway) and when the sample size is much greater than the number of parameters <span class="citation">(McElreath <a href="#ref-mcelreath2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>So our estimate for predictive performance given by AIC is the log-likelihood of the “best” parameter estimates, penalised by the number of model parameters. We do not have to worry about the factor “-2” in front of the log-likelihood,<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> but need to get used to the fact that smaller (possibly negative) values indicate better models. Also note that AIC is not an absolute measure of model out-of-sample performance, as we do not have an absolute benchmark like a true process; AIC only makes sense <em>relatively</em> when comparing two models. AIC is implemented in automated model selection algorithms, such as the <code>step()</code> function in <em>R</em>. Other information criteria exist but they are based on assumptions that, under the classic paradigm, are similar or even less realistic than those of AIC.</p>
<p>Let’s analyse the ozone dataset now. We start up-ward with just the individual predictors. Prior to that we standardise the predictors (see Chapter <a href="02-math.html#math">1</a>). This brings all predictors onto the same scale and hence makes parameter estimates comparable. It also makes parameters easier to interpret. The intercept now is the ozone concentration when all predictors are at their mean values. And each parameter measures the change in ozone concentration when that predictor changes by one standard deviation. Standardising the predictors is common practice <span class="citation">(Gelman, Hill, and Vehtari <a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="05-multiple_lin_reg.html#cb64-1"></a><span class="co"># standardise predictors</span></span>
<span id="cb64-2"><a href="05-multiple_lin_reg.html#cb64-2"></a>ozone_std &lt;-<span class="st"> </span>ozone</span>
<span id="cb64-3"><a href="05-multiple_lin_reg.html#cb64-3"></a>ozone_std<span class="op">$</span>rad &lt;-<span class="st"> </span>(ozone<span class="op">$</span>rad<span class="op">-</span><span class="kw">mean</span>(ozone<span class="op">$</span>rad))<span class="op">/</span><span class="kw">sd</span>(ozone<span class="op">$</span>rad)</span>
<span id="cb64-4"><a href="05-multiple_lin_reg.html#cb64-4"></a>ozone_std<span class="op">$</span>temp &lt;-<span class="st"> </span>(ozone<span class="op">$</span>temp<span class="op">-</span><span class="kw">mean</span>(ozone<span class="op">$</span>temp))<span class="op">/</span><span class="kw">sd</span>(ozone<span class="op">$</span>temp)</span>
<span id="cb64-5"><a href="05-multiple_lin_reg.html#cb64-5"></a>ozone_std<span class="op">$</span>wind &lt;-<span class="st"> </span>(ozone<span class="op">$</span>wind<span class="op">-</span><span class="kw">mean</span>(ozone<span class="op">$</span>wind))<span class="op">/</span><span class="kw">sd</span>(ozone<span class="op">$</span>wind)</span>
<span id="cb64-6"><a href="05-multiple_lin_reg.html#cb64-6"></a><span class="co"># multiple linear regression model with 3 predictors</span></span>
<span id="cb64-7"><a href="05-multiple_lin_reg.html#cb64-7"></a>ozone_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(ozone <span class="op">~</span><span class="st"> </span>rad<span class="op">+</span>temp<span class="op">+</span>wind, <span class="dt">data =</span> ozone_std)</span>
<span id="cb64-8"><a href="05-multiple_lin_reg.html#cb64-8"></a><span class="co"># extract information about parameter estimates</span></span>
<span id="cb64-9"><a href="05-multiple_lin_reg.html#cb64-9"></a><span class="kw">summary</span>(ozone_fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ozone ~ rad + temp + wind, data = ozone_std)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -40.49 -14.21  -3.56  10.12  95.60 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    42.10       2.01   20.95  &lt; 2e-16 ***
## rad             5.45       2.11    2.58    0.011 *  
## temp           15.74       2.42    6.52  2.4e-09 ***
## wind          -11.88       2.33   -5.10  1.4e-06 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.2 on 107 degrees of freedom
## Multiple R-squared:  0.606,  Adjusted R-squared:  0.595 
## F-statistic: 54.9 on 3 and 107 DF,  p-value: &lt;2e-16</code></pre>
<p>Before we interpret this, let’s look at the residuals:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="05-multiple_lin_reg.html#cb66-1"></a><span class="co"># residuals by index</span></span>
<span id="cb66-2"><a href="05-multiple_lin_reg.html#cb66-2"></a><span class="kw">plot</span>(<span class="kw">residuals</span>(ozone_fit), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb66-3"><a href="05-multiple_lin_reg.html#cb66-3"></a><span class="co"># residuals by modelled value</span></span>
<span id="cb66-4"><a href="05-multiple_lin_reg.html#cb66-4"></a><span class="kw">plot</span>(<span class="kw">fitted.values</span>(ozone_fit), <span class="kw">residuals</span>(ozone_fit), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb66-5"><a href="05-multiple_lin_reg.html#cb66-5"></a><span class="co"># residual histogram</span></span>
<span id="cb66-6"><a href="05-multiple_lin_reg.html#cb66-6"></a><span class="kw">hist</span>(<span class="kw">residuals</span>(ozone_fit))</span>
<span id="cb66-7"><a href="05-multiple_lin_reg.html#cb66-7"></a><span class="co"># residual QQ-plot</span></span>
<span id="cb66-8"><a href="05-multiple_lin_reg.html#cb66-8"></a><span class="kw">qqnorm</span>(<span class="kw">residuals</span>(ozone_fit))</span>
<span id="cb66-9"><a href="05-multiple_lin_reg.html#cb66-9"></a><span class="kw">qqline</span>(<span class="kw">residuals</span>(ozone_fit))</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-15-1.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-15-2.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-15-3.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-15-4.png" width="50%" /></p>
<p>The residuals are heteroscedastic and right-skewed, which is something we might be able to fix by log-transforming the response. This is something to try in any case when the response varies over orders of magnitude <span class="citation">(Gelman, Hill, and Vehtari <a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="05-multiple_lin_reg.html#cb67-1"></a><span class="co"># add log-transform of ozone to data.frame</span></span>
<span id="cb67-2"><a href="05-multiple_lin_reg.html#cb67-2"></a>ozone_std<span class="op">$</span>log_ozone &lt;-<span class="st"> </span><span class="kw">log</span>(ozone_std<span class="op">$</span>ozone)</span>
<span id="cb67-3"><a href="05-multiple_lin_reg.html#cb67-3"></a><span class="co"># fit</span></span>
<span id="cb67-4"><a href="05-multiple_lin_reg.html#cb67-4"></a>log_ozone_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(log_ozone <span class="op">~</span><span class="st"> </span>rad<span class="op">+</span>temp<span class="op">+</span>wind, <span class="dt">data =</span> ozone_std)</span>
<span id="cb67-5"><a href="05-multiple_lin_reg.html#cb67-5"></a><span class="kw">summary</span>(log_ozone_fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log_ozone ~ rad + temp + wind, data = ozone_std)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.0621 -0.2997 -0.0022  0.3077  1.2357 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.4159     0.0483   70.77  &lt; 2e-16 ***
## rad           0.2292     0.0507    4.52  1.6e-05 ***
## temp          0.4685     0.0580    8.08  1.1e-12 ***
## wind         -0.2192     0.0559   -3.92  0.00016 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.509 on 107 degrees of freedom
## Multiple R-squared:  0.665,  Adjusted R-squared:  0.655 
## F-statistic: 70.6 on 3 and 107 DF,  p-value: &lt;2e-16</code></pre>
<p>Log-transforming ozone stabilised the predictors and also increased <span class="math inline">\(r^2\)</span> by 0.05. The relationship of ozone to the three predictors has apparently turned more linear on the log-scale. The residuals, too, conform better to the assumptions, except for one outlier at the very low end:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="05-multiple_lin_reg.html#cb69-1"></a><span class="kw">plot</span>(<span class="kw">residuals</span>(log_ozone_fit), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb69-2"><a href="05-multiple_lin_reg.html#cb69-2"></a><span class="kw">plot</span>(<span class="kw">fitted.values</span>(log_ozone_fit), <span class="kw">residuals</span>(log_ozone_fit), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb69-3"><a href="05-multiple_lin_reg.html#cb69-3"></a><span class="kw">hist</span>(<span class="kw">residuals</span>(log_ozone_fit))</span>
<span id="cb69-4"><a href="05-multiple_lin_reg.html#cb69-4"></a><span class="kw">qqnorm</span>(<span class="kw">residuals</span>(log_ozone_fit))</span>
<span id="cb69-5"><a href="05-multiple_lin_reg.html#cb69-5"></a><span class="kw">qqline</span>(<span class="kw">residuals</span>(log_ozone_fit))</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-17-1.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-17-2.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-17-3.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-17-4.png" width="50%" /></p>
<p>Finally, the AIC of the log-ozone model got a lot better compared to the ozone model (remember that a smaller AIC indicates a better model):</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="05-multiple_lin_reg.html#cb70-1"></a><span class="kw">AIC</span>(ozone_fit)</span></code></pre></div>
<pre><code>## [1] 998.6</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="05-multiple_lin_reg.html#cb72-1"></a><span class="kw">AIC</span>(log_ozone_fit)</span></code></pre></div>
<pre><code>## [1] 170.8</code></pre>
<p>So we can proceed from this base model and see if adding interactions improves fit <span class="math inline">\(\left(r^2\right)\)</span> and predictive performance (AIC). We start by adding the interaction of the largest effects <span class="citation">(Gelman, Hill, and Vehtari <a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="05-multiple_lin_reg.html#cb74-1"></a>log_ozone_fit2 &lt;-<span class="st"> </span><span class="kw">lm</span>(log_ozone <span class="op">~</span><span class="st"> </span>rad<span class="op">*</span>temp<span class="op">+</span>wind, <span class="dt">data =</span> ozone_std)</span>
<span id="cb74-2"><a href="05-multiple_lin_reg.html#cb74-2"></a><span class="kw">summary</span>(log_ozone_fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log_ozone ~ rad * temp + wind, data = ozone_std)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.1824 -0.3019  0.0067  0.3124  1.1897 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.4022     0.0506   67.23  &lt; 2e-16 ***
## rad           0.2524     0.0568    4.44  2.2e-05 ***
## temp          0.4692     0.0581    8.08  1.1e-12 ***
## wind         -0.2193     0.0559   -3.92  0.00016 ***
## rad:temp      0.0472     0.0518    0.91  0.36444    
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.509 on 106 degrees of freedom
## Multiple R-squared:  0.667,  Adjusted R-squared:  0.655 
## F-statistic: 53.1 on 4 and 106 DF,  p-value: &lt;2e-16</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="05-multiple_lin_reg.html#cb76-1"></a><span class="kw">AIC</span>(log_ozone_fit2)</span></code></pre></div>
<pre><code>## [1] 171.9</code></pre>
<p>This interaction actually makes the predictive performance a little worse (greater AIC). Let’s try the next:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="05-multiple_lin_reg.html#cb78-1"></a>log_ozone_fit3 &lt;-<span class="st"> </span><span class="kw">lm</span>(log_ozone <span class="op">~</span><span class="st"> </span>rad<span class="op">+</span>temp<span class="op">*</span>wind, <span class="dt">data =</span> ozone_std)</span>
<span id="cb78-2"><a href="05-multiple_lin_reg.html#cb78-2"></a><span class="kw">summary</span>(log_ozone_fit3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log_ozone ~ rad + temp * wind, data = ozone_std)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9870 -0.3208 -0.0543  0.3024  1.1902 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.3670     0.0525   64.19  &lt; 2e-16 ***
## rad           0.2364     0.0500    4.73  6.9e-06 ***
## temp          0.4681     0.0570    8.21  5.7e-13 ***
## wind         -0.2198     0.0549   -4.00  0.00012 ***
## temp:wind    -0.0994     0.0454   -2.19  0.03097 *  
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5 on 106 degrees of freedom
## Multiple R-squared:  0.679,  Adjusted R-squared:  0.667 
## F-statistic: 56.1 on 4 and 106 DF,  p-value: &lt;2e-16</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="05-multiple_lin_reg.html#cb80-1"></a><span class="kw">AIC</span>(log_ozone_fit3)</span></code></pre></div>
<pre><code>## [1] 167.9</code></pre>
<p>Predictive performance is a little better than the base model (smaller AIC) and <span class="math inline">\(r^2\)</span> increased by 0.02, even if the interaction comes out fairly uncertain. On to the last interaction:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="05-multiple_lin_reg.html#cb82-1"></a>log_ozone_fit4 &lt;-<span class="st"> </span><span class="kw">lm</span>(log_ozone <span class="op">~</span><span class="st"> </span>rad<span class="op">*</span>wind<span class="op">+</span>temp, <span class="dt">data =</span> ozone_std)</span>
<span id="cb82-2"><a href="05-multiple_lin_reg.html#cb82-2"></a><span class="kw">summary</span>(log_ozone_fit4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log_ozone ~ rad * wind + temp, data = ozone_std)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9986 -0.3308 -0.0163  0.2664  1.2228 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.4065     0.0484   70.42  &lt; 2e-16 ***
## rad           0.2523     0.0527    4.79  5.4e-06 ***
## wind         -0.2169     0.0556   -3.90  0.00017 ***
## temp          0.4683     0.0577    8.12  8.9e-13 ***
## rad:wind     -0.0747     0.0492   -1.52  0.13175    
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.505 on 106 degrees of freedom
## Multiple R-squared:  0.672,  Adjusted R-squared:  0.659 
## F-statistic: 54.2 on 4 and 106 DF,  p-value: &lt;2e-16</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="05-multiple_lin_reg.html#cb84-1"></a><span class="kw">AIC</span>(log_ozone_fit4)</span></code></pre></div>
<pre><code>## [1] 170.4</code></pre>
<p>This interaction is not estimated precisely and we only gain an improvement in <span class="math inline">\(r^2\)</span> of 0.01 and a negligible gain in AIC. So I’m inclined to go with model 3 (all three predictors and the “temp:wind” interaction). Due to the imprecision of the interaction parameter it does not make sense to add more predictors at this stage. The residuals of model 3 still look ok:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="05-multiple_lin_reg.html#cb86-1"></a><span class="kw">plot</span>(<span class="kw">residuals</span>(log_ozone_fit3), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb86-2"><a href="05-multiple_lin_reg.html#cb86-2"></a><span class="kw">plot</span>(<span class="kw">fitted.values</span>(log_ozone_fit3), <span class="kw">residuals</span>(log_ozone_fit3), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb86-3"><a href="05-multiple_lin_reg.html#cb86-3"></a><span class="kw">hist</span>(<span class="kw">residuals</span>(log_ozone_fit3))</span>
<span id="cb86-4"><a href="05-multiple_lin_reg.html#cb86-4"></a><span class="kw">qqnorm</span>(<span class="kw">residuals</span>(log_ozone_fit3))</span>
<span id="cb86-5"><a href="05-multiple_lin_reg.html#cb86-5"></a><span class="kw">qqline</span>(<span class="kw">residuals</span>(log_ozone_fit3))</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-22-1.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-22-2.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-22-3.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-22-4.png" width="50%" /></p>
<p>The model that we settled with is:
<span class="math display" id="eq:ozonemod">\[\begin{equation}
\log(ozone)=3.4+0.2\cdot rad_{std}+0.5\cdot temp_{std}-0.2\cdot wind_{std}-0.1\cdot temp_{std}\cdot wind_{std}+\epsilon
\tag{4.2}
\end{equation}\]</span></p>
<p>It explains 68% of the variation in ozone concentration (at the log-scale, judged by <span class="math inline">\(r^2\)</span>) - which is pretty good - and tells us the following: The most important driver (of those we had data for) of ozone concentration is air temperature, followed by radiation intensity and wind speed. We get this from comparing the size of the parameters of the different predictors, which we can only do if predictors are standardised. Air temperature and radiation intensity increase ozone concentrations, while wind speed decreases it. The negative interaction of air temperature and wind speed tells us that the air temperature effects is down-regulated with increasing wind speeds.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> We can thus group Equation <a href="05-multiple_lin_reg.html#eq:ozonemod">(4.2)</a> as follows:
<span class="math display" id="eq:ozonemod2">\[\begin{equation}
\log(ozone)=3.4+0.2\cdot rad_{std}+\left(0.5-0.1\cdot wind_{std}\right)\cdot temp_{std}-0.2\cdot wind_{std}+\epsilon
\tag{4.3}
\end{equation}\]</span></p>
<p>This is a useful way of understanding interactions, which we will revisit now in the last section.</p>
</div>
<div id="ancova" class="section level2">
<h2><span class="header-section-number">4.5</span> Mixed continuous-categorical predictors</h2>
<p>The special case of a mix of <em>continuous and categorical predictors</em> has got the special name of <strong>analysis of covariance</strong> (ANCOVA), though as we said before (chapter <a href="04-categorical_vars.html#categoricalvars">3</a>) it is really just another variant of a linear model. For illustration we use an example from <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>, modelling childrens’ IQ score by their mothers’ IQ score and whether or not the mothers have a high school degree (Figure <a href="05-multiple_lin_reg.html#fig:iqdat">4.5</a>).</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="05-multiple_lin_reg.html#cb87-1"></a><span class="co"># load IQ data from remote repository</span></span>
<span id="cb87-2"><a href="05-multiple_lin_reg.html#cb87-2"></a>iq &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/KidIQ/data/kidiq.csv&quot;</span>, <span class="dt">header=</span><span class="ot">TRUE</span>)</span>
<span id="cb87-3"><a href="05-multiple_lin_reg.html#cb87-3"></a><span class="co"># plot kid&#39;s IQ against mom&#39;s IQ</span></span>
<span id="cb87-4"><a href="05-multiple_lin_reg.html#cb87-4"></a><span class="co"># with symbols differentiated by whether or not the mom has a high school degree</span></span>
<span id="cb87-5"><a href="05-multiple_lin_reg.html#cb87-5"></a><span class="kw">plot</span>(iq<span class="op">$</span>mom_iq, iq<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb87-6"><a href="05-multiple_lin_reg.html#cb87-6"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb87-7"><a href="05-multiple_lin_reg.html#cb87-7"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:iqdat"></span>
<img src="qm4g_files/figure-html/iqdat-1.png" alt="Childrens' IQ score against their mothers' IQ score, with symbol and shading indicating whether or not the mothers have a high school degree (open black dots: no high school degree; closed grey dots: highschool degree). Data from: @gelman2020" width="80%" />
<p class="caption">
Figure 4.5: Childrens’ IQ score against their mothers’ IQ score, with symbol and shading indicating whether or not the mothers have a high school degree (open black dots: no high school degree; closed grey dots: highschool degree). Data from: <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>
</p>
</div>
<p>We first centre the predictor “mom_iq” (the IQ score of the mothers) to make the corresponding parameter easier to interpret. Note, standardisation is not necessary here because there is no other continuous predictor to compare against, just a binary predictor.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> The binary predictor is “mom_hs”, with “1” indicating mother has a high school degree and “0” indicating mother has not got one.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="05-multiple_lin_reg.html#cb88-1"></a><span class="co"># centre continuous predictor</span></span>
<span id="cb88-2"><a href="05-multiple_lin_reg.html#cb88-2"></a>iq_cen &lt;-<span class="st"> </span>iq</span>
<span id="cb88-3"><a href="05-multiple_lin_reg.html#cb88-3"></a>iq_cen<span class="op">$</span>mom_iq &lt;-<span class="st"> </span>iq<span class="op">$</span>mom_iq<span class="op">-</span><span class="kw">mean</span>(iq<span class="op">$</span>mom_iq)</span></code></pre></div>
<p>We then fit all model variants with and without interaction all at once and compare them via AIC. Normally we would do this step by step as in the previous example and check residuals at every stage. I skip this here and only check residuals at the end because I am more interested in showcasing the different model variants and what they mean mathematically and mechanistically.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="05-multiple_lin_reg.html#cb89-1"></a><span class="co"># fit Null model</span></span>
<span id="cb89-2"><a href="05-multiple_lin_reg.html#cb89-2"></a>iq_fit0 &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> iq_cen)</span>
<span id="cb89-3"><a href="05-multiple_lin_reg.html#cb89-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(iq_fit0))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value   Pr(&gt;|t|)
## (Intercept)     86.8     0.9797   88.59 1.331e-279</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="05-multiple_lin_reg.html#cb91-1"></a><span class="kw">AIC</span>(iq_fit0)</span></code></pre></div>
<pre><code>## [1] 3853</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="05-multiple_lin_reg.html#cb93-1"></a><span class="co"># fit common slope and intercept model</span></span>
<span id="cb93-2"><a href="05-multiple_lin_reg.html#cb93-2"></a>iq_fit1 &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq, <span class="dt">data =</span> iq_cen)</span>
<span id="cb93-3"><a href="05-multiple_lin_reg.html#cb93-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(iq_fit1))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value   Pr(&gt;|t|)
## (Intercept)    86.80    0.87680   98.99 5.127e-299
## mom_iq          0.61    0.05852   10.42  7.662e-23</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="05-multiple_lin_reg.html#cb95-1"></a><span class="kw">AIC</span>(iq_fit1)</span></code></pre></div>
<pre><code>## [1] 3757</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="05-multiple_lin_reg.html#cb97-1"></a><span class="co"># fit individual Null models (2 intercepts)</span></span>
<span id="cb97-2"><a href="05-multiple_lin_reg.html#cb97-2"></a>iq_fit2 &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_hs, <span class="dt">data =</span> iq_cen)</span>
<span id="cb97-3"><a href="05-multiple_lin_reg.html#cb97-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(iq_fit2))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value   Pr(&gt;|t|)
## (Intercept)    77.55      2.059  37.670 1.392e-138
## mom_hs         11.77      2.322   5.069  5.957e-07</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="05-multiple_lin_reg.html#cb99-1"></a><span class="kw">AIC</span>(iq_fit2)</span></code></pre></div>
<pre><code>## [1] 3830</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="05-multiple_lin_reg.html#cb101-1"></a><span class="co"># fit common slope model (2 intercepts)</span></span>
<span id="cb101-2"><a href="05-multiple_lin_reg.html#cb101-2"></a>iq_fit3 &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq<span class="op">+</span>mom_hs, <span class="dt">data =</span> iq_cen)</span>
<span id="cb101-3"><a href="05-multiple_lin_reg.html#cb101-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(iq_fit3))</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value   Pr(&gt;|t|)
## (Intercept)  82.1221    1.94370  42.250 2.436e-155
## mom_iq        0.5639    0.06057   9.309  6.610e-19
## mom_hs        5.9501    2.21181   2.690  7.419e-03</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="05-multiple_lin_reg.html#cb103-1"></a><span class="kw">AIC</span>(iq_fit3)</span></code></pre></div>
<pre><code>## [1] 3752</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="05-multiple_lin_reg.html#cb105-1"></a><span class="co"># fit common intercept model (2 slopes)</span></span>
<span id="cb105-2"><a href="05-multiple_lin_reg.html#cb105-2"></a>iq_fit4 &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq<span class="op">+</span>mom_iq<span class="op">:</span>mom_hs, <span class="dt">data =</span> iq_cen)</span>
<span id="cb105-3"><a href="05-multiple_lin_reg.html#cb105-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(iq_fit4))</span></code></pre></div>
<pre><code>##               Estimate Std. Error t value   Pr(&gt;|t|)
## (Intercept)    87.7806     0.8998  97.551 6.603e-296
## mom_iq          1.0550     0.1289   8.186  3.085e-15
## mom_iq:mom_hs  -0.5658     0.1466  -3.860  1.308e-04</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="05-multiple_lin_reg.html#cb107-1"></a><span class="kw">AIC</span>(iq_fit4)</span></code></pre></div>
<pre><code>## [1] 3744</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="05-multiple_lin_reg.html#cb109-1"></a><span class="co"># fit maximal model</span></span>
<span id="cb109-2"><a href="05-multiple_lin_reg.html#cb109-2"></a>iq_fit5 &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq<span class="op">*</span>mom_hs, <span class="dt">data =</span> iq_cen)</span>
<span id="cb109-3"><a href="05-multiple_lin_reg.html#cb109-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(iq_fit5))</span></code></pre></div>
<pre><code>##               Estimate Std. Error t value   Pr(&gt;|t|)
## (Intercept)    85.4069     2.2182  38.502 1.970e-141
## mom_iq          0.9689     0.1483   6.531  1.843e-10
## mom_hs          2.8408     2.4267   1.171  2.424e-01
## mom_iq:mom_hs  -0.4843     0.1622  -2.985  2.994e-03</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="05-multiple_lin_reg.html#cb111-1"></a><span class="kw">AIC</span>(iq_fit5)</span></code></pre></div>
<pre><code>## [1] 3745</code></pre>
<p>The most complex model (#5) comes out on top here according to AIC. Its residuals look ok too:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="05-multiple_lin_reg.html#cb113-1"></a><span class="kw">plot</span>(<span class="kw">residuals</span>(iq_fit5), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb113-2"><a href="05-multiple_lin_reg.html#cb113-2"></a><span class="kw">plot</span>(<span class="kw">fitted.values</span>(iq_fit5), <span class="kw">residuals</span>(iq_fit5), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb113-3"><a href="05-multiple_lin_reg.html#cb113-3"></a><span class="kw">hist</span>(<span class="kw">residuals</span>(iq_fit5))</span>
<span id="cb113-4"><a href="05-multiple_lin_reg.html#cb113-4"></a><span class="kw">qqnorm</span>(<span class="kw">residuals</span>(iq_fit5))</span>
<span id="cb113-5"><a href="05-multiple_lin_reg.html#cb113-5"></a><span class="kw">qqline</span>(<span class="kw">residuals</span>(iq_fit5))</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-25-1.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-25-2.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-25-3.png" width="50%" /><img src="qm4g_files/figure-html/unnamed-chunk-25-4.png" width="50%" /></p>
<p>Let’s plot the six variants to look at the meaning of interactions once more (Figure <a href="05-multiple_lin_reg.html#fig:iqmod">4.6</a>).</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="05-multiple_lin_reg.html#cb114-1"></a><span class="co"># Null model</span></span>
<span id="cb114-2"><a href="05-multiple_lin_reg.html#cb114-2"></a><span class="kw">plot</span>(iq_cen<span class="op">$</span>mom_iq, iq_cen<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-3"><a href="05-multiple_lin_reg.html#cb114-3"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-4"><a href="05-multiple_lin_reg.html#cb114-4"></a>     <span class="dt">main =</span><span class="st">&quot;Null model&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ (centred)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span>
<span id="cb114-5"><a href="05-multiple_lin_reg.html#cb114-5"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">coef</span>(iq_fit0), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb114-6"><a href="05-multiple_lin_reg.html#cb114-6"></a><span class="co"># common slope and intercept</span></span>
<span id="cb114-7"><a href="05-multiple_lin_reg.html#cb114-7"></a><span class="kw">plot</span>(iq_cen<span class="op">$</span>mom_iq, iq_cen<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-8"><a href="05-multiple_lin_reg.html#cb114-8"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-9"><a href="05-multiple_lin_reg.html#cb114-9"></a>     <span class="dt">main =</span> <span class="st">&quot;common slope and intercept&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ (centred)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span>
<span id="cb114-10"><a href="05-multiple_lin_reg.html#cb114-10"></a><span class="kw">abline</span>(<span class="kw">coef</span>(iq_fit1), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb114-11"><a href="05-multiple_lin_reg.html#cb114-11"></a><span class="co"># individual Null models</span></span>
<span id="cb114-12"><a href="05-multiple_lin_reg.html#cb114-12"></a><span class="kw">plot</span>(iq_cen<span class="op">$</span>mom_iq, iq_cen<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-13"><a href="05-multiple_lin_reg.html#cb114-13"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-14"><a href="05-multiple_lin_reg.html#cb114-14"></a>     <span class="dt">main =</span> <span class="st">&quot;individual Null models&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ (centred)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span>
<span id="cb114-15"><a href="05-multiple_lin_reg.html#cb114-15"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">coef</span>(iq_fit2)[<span class="dv">1</span>], <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb114-16"><a href="05-multiple_lin_reg.html#cb114-16"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">sum</span>(<span class="kw">coef</span>(iq_fit2)), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb114-17"><a href="05-multiple_lin_reg.html#cb114-17"></a><span class="co"># common slope</span></span>
<span id="cb114-18"><a href="05-multiple_lin_reg.html#cb114-18"></a><span class="kw">plot</span>(iq_cen<span class="op">$</span>mom_iq, iq_cen<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-19"><a href="05-multiple_lin_reg.html#cb114-19"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-20"><a href="05-multiple_lin_reg.html#cb114-20"></a>     <span class="dt">main =</span> <span class="st">&quot;common slope&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ (centred)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span>
<span id="cb114-21"><a href="05-multiple_lin_reg.html#cb114-21"></a><span class="kw">abline</span>(<span class="kw">coef</span>(iq_fit3)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)], <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb114-22"><a href="05-multiple_lin_reg.html#cb114-22"></a><span class="kw">abline</span>(<span class="kw">c</span>(<span class="kw">sum</span>(<span class="kw">coef</span>(iq_fit3)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)]),<span class="kw">coef</span>(iq_fit3)[<span class="dv">2</span>]), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb114-23"><a href="05-multiple_lin_reg.html#cb114-23"></a><span class="co"># common intercept</span></span>
<span id="cb114-24"><a href="05-multiple_lin_reg.html#cb114-24"></a><span class="kw">plot</span>(iq_cen<span class="op">$</span>mom_iq, iq_cen<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-25"><a href="05-multiple_lin_reg.html#cb114-25"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-26"><a href="05-multiple_lin_reg.html#cb114-26"></a>     <span class="dt">main =</span> <span class="st">&quot;common intercept&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ (centred)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span>
<span id="cb114-27"><a href="05-multiple_lin_reg.html#cb114-27"></a><span class="kw">abline</span>(<span class="kw">coef</span>(iq_fit4)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)], <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb114-28"><a href="05-multiple_lin_reg.html#cb114-28"></a><span class="kw">abline</span>(<span class="kw">c</span>(<span class="kw">coef</span>(iq_fit4)[<span class="dv">1</span>],<span class="kw">sum</span>(<span class="kw">coef</span>(iq_fit4)[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)])), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb114-29"><a href="05-multiple_lin_reg.html#cb114-29"></a><span class="co"># maximal model</span></span>
<span id="cb114-30"><a href="05-multiple_lin_reg.html#cb114-30"></a><span class="kw">plot</span>(iq_cen<span class="op">$</span>mom_iq, iq_cen<span class="op">$</span>kid_score, <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">20</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-31"><a href="05-multiple_lin_reg.html#cb114-31"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;gray&quot;</span>)[iq_cen<span class="op">$</span>mom_hs<span class="op">+</span><span class="dv">1</span>],</span>
<span id="cb114-32"><a href="05-multiple_lin_reg.html#cb114-32"></a>     <span class="dt">main =</span> <span class="st">&quot;maximal model&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Mom&#39;s IQ (centred)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Kid&#39;s IQ&quot;</span>)</span>
<span id="cb114-33"><a href="05-multiple_lin_reg.html#cb114-33"></a><span class="kw">abline</span>(<span class="kw">coef</span>(iq_fit5)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)], <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb114-34"><a href="05-multiple_lin_reg.html#cb114-34"></a><span class="kw">abline</span>(<span class="kw">c</span>(<span class="kw">sum</span>(<span class="kw">coef</span>(iq_fit5)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)]),<span class="kw">sum</span>(<span class="kw">coef</span>(iq_fit5)[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)])), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:iqmod"></span>
<img src="qm4g_files/figure-html/iqmod-1.png" alt="Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: @gelman2020." width="33%" /><img src="qm4g_files/figure-html/iqmod-2.png" alt="Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: @gelman2020." width="33%" /><img src="qm4g_files/figure-html/iqmod-3.png" alt="Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: @gelman2020." width="33%" /><img src="qm4g_files/figure-html/iqmod-4.png" alt="Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: @gelman2020." width="33%" /><img src="qm4g_files/figure-html/iqmod-5.png" alt="Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: @gelman2020." width="33%" /><img src="qm4g_files/figure-html/iqmod-6.png" alt="Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: @gelman2020." width="33%" />
<p class="caption">
Figure 4.6: Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>.
</p>
</div>
<p>Mathematically, the Null model is:
<span class="math display" id="eq:ancova1">\[\begin{equation}
IQ_{kid}=87+\epsilon
\tag{4.4}
\end{equation}\]</span>
I.e. the Null model is nothing more than the children’s mean IQ score, which is 87.</p>
<p>The common slope and intercept model is:
<span class="math display" id="eq:ancova2">\[\begin{equation}
IQ_{kid}=87+0.6\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)+\epsilon
\tag{4.5}
\end{equation}\]</span>
So when mother’s IQ score is at its average then the child’s IQ score is again the overall average, 87, but increases or decreases by 0.6 for every unit increase or decrease in mother’s IQ score.</p>
<p>The individual Null models formulation is:
<span class="math display" id="eq:ancova3">\[\begin{equation}
IQ_{kid}=78+12\cdot hs_{mom}+\epsilon
\tag{4.6}
\end{equation}\]</span>
These are two different intercepts now, the mean children’s IQ score for mothers without a high school degree (when <span class="math inline">\(hs_{mom}=0\)</span>), which is 78, and the mean children’s IQ score for mothers with a high school degree (when <span class="math inline">\(hs_{mom}=1\)</span>), which is 78+12=90.</p>
<p>The common slope model is:
<span class="math display" id="eq:ancova4">\[\begin{equation}
IQ_{kid}=82+6\cdot hs_{mom}+0.6\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)+\epsilon
\tag{4.7}
\end{equation}\]</span>
This model assumes different mean children’s IQ scores for mothers with and without high school degree, but the same dependence on the mothers’ IQ score, which again comes out at 0.6.</p>
<p>The common intercept model now includes the interaction term, but not <span class="math inline">\(hs_{mom}\)</span> as an individual predictor:
<span class="math display" id="eq:ancova5">\[\begin{equation}
IQ_{kid}=88+1.1\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)-0.6\cdot hs_{mom}\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)+\epsilon
\tag{4.8}
\end{equation}\]</span></p>
<p>Regrouping leads to:
<span class="math display" id="eq:ancova5b">\[\begin{equation}
IQ_{kid}=88+\left(1.1-0.6\cdot hs_{mom}\right)\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)+\epsilon
\tag{4.9}
\end{equation}\]</span>
According to this model we embark from a common intercept but then follow different slopes depending on mother’s high school degree. When mother has a high school degree, the child’s IQ increases by 1.1-0.6=0.5 for every unit increase in mother’s IQ. When mother has no high school degree, then mother’s IQ effect is stronger, increasing the child’s IQ by 1.1 for very unit increase in mother’s IQ.</p>
<p>Finally, the maximal model includes all predictors:
<span class="math display" id="eq:ancova6">\[\begin{equation}
IQ_{kid}=85+3\cdot hs_{mom}+1.0\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)-0.5\cdot hs_{mom}\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)+\epsilon
\tag{4.10}
\end{equation}\]</span></p>
<p>Regrouping leads to:
<span class="math display" id="eq:ancova6b">\[\begin{equation}
IQ_{kid}=85+3\cdot hs_{mom}+\left(1.0-0.5\cdot hs_{mom}\right)\cdot \left(IQ_{mom}-\bar{IQ}_{mom}\right)+\epsilon
\tag{4.11}
\end{equation}\]</span>
So positing two different average children’s IQ scores for mothers with and without high school degree, 88 and 85 respectively, and two different dependencies on mothers’ IQ score, 0.5 and 1.0 respectively. If we believe this model that turned out best according to AIC, then there is a difference in average children’s IQ score depending on whether or not the children’s mothers have a high school degree (with high school degrees improving IQ scores by 3 units on average). What also matters is the mothers’ IQ score, with a positive relationship between mothers’ and children’s score. This relationship is stronger (mother’s IQ matters more) when mothers have no high school degree than when they have one.</p>
</div>
<div id="general-advise" class="section level2">
<h2><span class="header-section-number">4.6</span> General advise</h2>
<p>Let’s finish with some general advise for building regression models, taken from <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>:</p>
<ul>
<li>Include all predictors that we consider important for mechanistic reasons.</li>
<li>Consider combining several predictors into a “total score” by averaging or summation (this is something we have not looked at so far).</li>
<li>If predictors have large effects, consider including their interactions.</li>
<li>Use standard errors to get a sense of uncertainties in parameter estimates.</li>
<li>Decide upon including or excluding predictors based on a combination of contextual understanding, data, and the uses to which the regression will be put:
<ul>
<li>If the parameter of a predictor is estimated precisely (small standard error), then it generally makes sense to keep it in the model as it should improve predictions.</li>
<li>If the standard error of a parameter is large and there is no good mechanistic reason for the predictor to be included, then it can make sense to remove it, as this can allow the other model parameters to be estimated more stably and can even reduce prediction errors.</li>
<li>If a predictor is important for the problem at hand, then <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span> generally recommend keeping it in, even if the estimate has a large standard error and is not “statistically significant”. In such settings we must acknowledge the resulting uncertainty and perhaps try to reduce it, e.g. by gathering more data.</li>
<li>If a coefficient does not make sense, then we should try to understand how this could happen. If the standard error is large, the estimate could be explainable from random variation. If the standard error is small, it can make sense to put more effort into understanding the coefficient.</li>
</ul></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-crawley2012">
<p>Crawley, M. J. 2012. <em>The R Book (2nd Ed.)</em>. Chichester: John Wiley &amp; Sons Ltd.</p>
</div>
<div id="ref-gelman2020">
<p>Gelman, A., J. Hill, and A. Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-mcelreath2020">
<p>McElreath, R. 2020. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd Ed.)</em>. Boca Raton: CRC Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>This is vividly portrayed in Umberto Eco’s book “The name of the rose”.<a href="05-multiple_lin_reg.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>See chapter <a href="06-ml_bayes.html#mlbayes">5</a> for the alternative Bayesian statistical paradigm.<a href="05-multiple_lin_reg.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The answer can be found at the end of this script.<a href="05-multiple_lin_reg.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Under the Bayesian statistical paradigm, better choices are available; we will look at those in my <em>Applied Statistical Modelling</em> course in the summer term.<a href="05-multiple_lin_reg.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>This just scales the log-likelihood so that a log-likelihood <em>difference</em> of two competing models has an asymptotic sampling distribution, which is a Chi-squared distribution in this case (see, for example, <span class="citation">McElreath (<a href="#ref-mcelreath2020" role="doc-biblioref">2020</a>)</span> and references therein).<a href="05-multiple_lin_reg.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>This way round seems to make more sense mechanistically than air temperature regulating the wind speed effect, although mathematically both are equivalent.<a href="05-multiple_lin_reg.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>We can also standardise binary predictors (see <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>, for example), but in my mind this complicates interpretation. So I prefer to leave binary predictors on their original scale, at the expense of not being able to judge relative importance of predictors.<a href="05-multiple_lin_reg.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="04-categorical_vars.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="06-ml_bayes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
